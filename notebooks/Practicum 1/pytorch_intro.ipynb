{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb07947b",
   "metadata": {},
   "source": [
    "# Введение в Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae15d0",
   "metadata": {},
   "source": [
    "\n",
    "Этот ноутбук - туториал по реализации пайплайна обучения и основным подмодулям Pytorch и другим полезным вещам. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302150d",
   "metadata": {},
   "source": [
    "# Импорт необходимых модулей\n",
    "В этом курсе я буду пользоваться Pytorch и Pytorch Geometric. Прелесть второй либы в том, что основные интерфейсы не отличаются от Pytorch. \n",
    "torch - основная либа\n",
    "Импортируем torch.nn - тут находятся классы, реализующие слои, лоссы, функции ошибок и тд. \n",
    "Для примера я использую стандартный датасет для компьютерного зрения - MNIST - содержащий цифры от 0 до 9 в разном написании. Поэтому импортируются некоторые подмодули torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bf7c9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "45ad1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aebe5f",
   "metadata": {},
   "source": [
    "# Создание датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d937ef",
   "metadata": {},
   "source": [
    "Pytorch позволяет использовать открытые датасеты с хаба. Они все являются подклассами `torch.utils.data.Dataset` и имплементируют методы, специфичные для этого класса данных. Обычно они используются для прототипирования и бенчмаркинга и могут быть загружены по необходимости. Однако если для задачи необзодимо что-то кроме предполагаемой разработчиками функциональности, нужно либо отнаследовать один из них, либо напрямую `torch.utils.data.Dataset`.\n",
    "\n",
    "Давайте загрузим FashionMNIST (https://github.com/zalandoresearch/fashion-mnist) - один из таких простых бенчмарк-датасетов для компьютерного зрения. Каждый семпл - это черно-белое изображение размером 28х28 с каким-то предметом одежды и ассоциированным классом. Всего набор содержит 60000 + 10000 изображений и 10 классов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "84744d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cdfc853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dc9d3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(data, labels_map, cols=3, rows=3):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(len(data), size=(1,)).item()\n",
    "        img, label = data[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        if type(label) == torch.Tensor:\n",
    "            plt.title(labels_map[label.item()])\n",
    "        else:\n",
    "            plt.title(labels_map[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6f1c8745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_samples(train_data, labels_map, cols=3, rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a33dc57",
   "metadata": {},
   "source": [
    "Каждый датасет в Pytorch должен наследоваться от класса Dataset и обязательно имплементировать методы `__init__, __len__, __getitem__`. В `__init__` обычно кроме прочего определяются трансформации, которые будут применяться к данным. Это может быть преобразование PIL.Image в тензоры, добавление шума, сдвиги и т.д. \n",
    "Pytorch предоставляет большое количество трансформаций, но наиболее популярная и удобная библиотека, реализующая трансформации для изображений - *albumentations*. \n",
    "\n",
    "`__len__` - должна возвращать размер датасета (например, чисо строк в файле с аннотациями)\n",
    "\n",
    "`__getitem__` - должен возвращать семпл данных, например, вектор признаков и y. Однако он может возвращать еще и дополнительную информацию, например, y может состоять из набора bboxoв и  сегментационных масок для детекции, класса изображения итп. Обычно возвращается tuple. В целом, возвращаться может любая структура, которую можно представить как набор тензоров, соответствующих одному семплу и входу модели.\n",
    "Внутри этого метода может происходить загрузка семпла, его трансформации и другие необходимые манипуляции. Очевидно, что он может возвращать не только изображение, но и любые другие данные, и временные ряды, и табличные данные, и облака точек, и другое. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d78bb",
   "metadata": {},
   "source": [
    "Давайте реализуем такой датасет для FashionMNIST. Конечно, можно было бы спокойно использовать встроенный, но так не интересно )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9022c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.mnist import read_image_file, read_label_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5df545a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, file_dir, train=True, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        image_file = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte\"\n",
    "        self.data = read_image_file(os.path.join(file_dir, image_file))\n",
    "\n",
    "        label_file = f\"{'train' if self.train else 't10k'}-labels-idx1-ubyte\"\n",
    "        self.targets = read_label_file(os.path.join(file_dir, label_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3d648b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNISTDataset(\n",
    "    file_dir=\"data/FashionMNIST/raw\",\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "test_data = FashionMNISTDataset(\n",
    "    file_dir=\"data/FashionMNIST/raw\",\n",
    "    train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d8e5055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_samples(train_data, labels_map, cols=3, rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7354c09",
   "metadata": {},
   "source": [
    "Датасет возвращает один семпл за раз, однако часто наши мощности позволяют обрабатывать больше данных. При работе с таблицами обыно модель получает на вход вообще весь набор сразу или достаточно большими порциями. Когда речь заходит о тензорах и наборах с сотнями тысяч и даже миллионами семплов, это невозможно. На помощь приходит батчевая обработка, когда из датасета берутся небольшие порции данных за раз (используя индекс). За соединение семплов во вход модели отвечает класс DataLoader. Каждую эпоху данные шаффлятся, чтобы уменьшить переобучение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6920b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "15214288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze().reshape(28,28)\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf84e10",
   "metadata": {},
   "source": [
    "Глубокое обучение чаще всего связывают с GPU, и не зря. Имея дело с тензорами и тензорными вычислениями, мы производим огромное количество элементарных операций. Каждое ядро CPU способно произвести только одно вычисление за такт. GPU был создан для обработки графики, где требуется также большое количество простых вычислений, и поэтому может ускорить обработку данных в тысячи раз. Это все должно быть хорошо Вам известно из курсов Корхова) \n",
    "\n",
    "К сожалению, часто компьютер не поддерживает вычисления на GPU, например, у него просто нет дискретной видеокарты. Pytorch позволяет проводить все операции на CPU, так что его можно использовать где угодно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "188d5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77c6be",
   "metadata": {},
   "source": [
    "*Дополнительное задание: адаптируйте весь код для `accelerate`. Это очень удобная библиотека для адаптации PyTorch кода для любой распределенной конфигурации. Кроме того, она позволяет упростить работу, даже если нам нужно перенести вычисления на GPU или CPU, автоматизируя этот процесс (иначе нам нужно постоянно вызывать `.to(device)`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9aa77",
   "metadata": {},
   "source": [
    "Теперь создадим класс модели и интстанциируем её. В данном случае это простой перцептрон с тремя слоями. Однако модель может быть сколько угодно сложной. Главное, чтобы она наследовала класс `nn.Module`. Более того, можно написать любой слой собственноручно, также отнаслеовав этот класс. \n",
    "Каждая модель должна имплементировать два метода - `__init__` и `forward(self, *args, **kwargs)`. В первом обычно задается ее структура и другие параметры. Модель  может даже содержать другие модели!\n",
    "\n",
    "`forward` получает на вход Х, но он может получать несколько разных входов, маски и тд. Обычно возвращаются либо выходы с последнего слоя (один или несколько). Но, например, стандартыне модели детекции возвращают в режиме обучения значение лосса, а непосредственные предсказания только в режиме тестирования.\n",
    "\n",
    "Вызывая `model(data)`, на самом деле мы вызываем именно метод `forward`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e538f",
   "metadata": {},
   "source": [
    "Обратите внимание, что PyTorch работает с тензорами, которые являются n-мерными массивами, функционально аналогичными ndarrays numpy, с дополнительной особенностью, что они могут быть использованы для вычислений на GPU. Подробнее о них можно прочитать здесь:\n",
    "\n",
    "Для того, чтобы понять, как устроен слой в Pytorch, создадим простой слой для превращения изображения в вектор. \n",
    "Пусть это будет класс MyFlatten. Изображения (и, что более актуально, промежуточные представления или, иными словами, карты, внутри модели) изначально имеют размер N x C x H x W, где:\n",
    "\n",
    "N - число семплов в батче\n",
    "C - число каналов на входе в конкретный слой. Если вход с 3 каналами, это 3, но в середине модели это может быть любое число, например, 128.\n",
    "H - высота карты\n",
    "W - ширина карты\n",
    "\n",
    "Когда мы делаем что-то вроде двумерной свертки, это стандартное представление данных, которое позволяет восстановить пространственные зависимости между элементами карты. Однако, когда мы используем полносвязные модели, каждый семпл представлен единым вектором. В простом случае больше не нужно разделять разные каналы, строки и столбцы данных. Таким образом, мы используем операцию \"вытянуть в вектор\", чтобы превратить значения C x H x W в представление C * H * W. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4040a",
   "metadata": {},
   "source": [
    "Задание: допишите класс MyFlatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6874d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFlatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.flatten(1, -1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "276275b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = MyFlatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4083c3",
   "metadata": {},
   "source": [
    "Слои: http://pytorch.org/docs/nn.html\n",
    "\n",
    "Активации: http://pytorch.org/docs/nn.html#non-linear-activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba191e",
   "metadata": {},
   "source": [
    "Зададим функцию ошибки и оптимизатор. Функция ошибки также наследуется от torch.nn.Module и может быть стандартной, состоять как из нескольких стандартных функций или их модификаций. Большое количество функций ошибки уже реализовано в Pytorch.\n",
    "\n",
    "Внутри каждой итерации обучения оптимизация происходит в три этапа:\n",
    "Вызов optimizer.zero_grad() для обнуления градиентов параметров модели. Градиенты по умолчанию складываются; чтобы избежать двойного подсчета, мы явно обнуляем их на каждой итерации.\n",
    "\n",
    "Получение градиентов для каждого веса с помощью loss.backward(). Подробнее об этом я расскажу на следующей лекции. \n",
    "Как только мы получили наши градиенты, остается вызвать optimizer.step(), чтобы изменить параметры в соответствии с нашим алгоритмом оптимизации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bd7b620e-da8d-4aad-9683-70a105788628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SGD(model):\n",
    "    return torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def get_Adam(model):\n",
    "     return torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def get_Adadelta(model):\n",
    "    return torch.optim.Adadelta(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eec223a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [nn.CrossEntropyLoss(), nn.NLLLoss(), nn.NLLLoss2d()]\n",
    "\n",
    "optimizers = [\n",
    "    get_SGD,\n",
    "    get_Adam,\n",
    "    get_Adadelta,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae01c0",
   "metadata": {},
   "source": [
    "Задание: \n",
    "1) Выберите как минимум еще 3 разные функции ошибки из https://pytorch.org/docs/stable/nn.html#loss-functions и обучите модель с ними\n",
    "2) Выберите как минимум еще 2 оптимизатора из https://pytorch.org/docs/stable/optim.html и обучите каждую из моделей с ним\n",
    "\n",
    "В результате у вас должна получиться таблицы с лучшими значениями лосса для каждого случая для обучения и теста. В идеале оформите их в виде pandas.DataFrame или другой таблицы с удобным выводом. На паре 13 марта обсудим, какие комбинации были лучшими, а какие худшими. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075289cb",
   "metadata": {},
   "source": [
    "Обучим модель. Для этого нужно описать цикл обучения и проверки модели. В наивном случае создается две функции train и validate (test), которые вызываются в цикле по эпохам. В дальнейшем, при усложнении прохода по эпохе, может появиться метод train, целый класс Trainer и т.д. Пока что реализуем наивный вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8931bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        preds = model(inputs)\n",
    "        loss = loss_fn(preds, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            current_batch_size = len(inputs)\n",
    "            loss, current = loss.item(), (batch_idx + 1) * current_batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "289e5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, num_correct = 0, 0\n",
    "    running_accuracy = 0 \n",
    "    total = 0 \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            preds = model(inputs)\n",
    "            test_loss += loss_fn(preds, targets).item()\n",
    "            num_correct += (preds.argmax(1) == targets).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    num_correct /= size\n",
    "    acc = 100*num_correct\n",
    "    print(f\"Test Error: \\n Accuracy: {acc:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return abs(test_loss), acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873bd8ea",
   "metadata": {},
   "source": [
    "Задание: Добавьте метрики precision и recall для проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc99a7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.376169  [   64/60000]\n",
      "loss: 1.468681  [ 6464/60000]\n",
      "loss: 1.058800  [12864/60000]\n",
      "loss: 1.139079  [19264/60000]\n",
      "loss: 0.787113  [25664/60000]\n",
      "loss: 0.843381  [32064/60000]\n",
      "loss: 0.796472  [38464/60000]\n",
      "loss: 0.683167  [44864/60000]\n",
      "loss: 0.695399  [51264/60000]\n",
      "loss: 0.785303  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.677383 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.864784  [   64/60000]\n",
      "loss: 0.574647  [ 6464/60000]\n",
      "loss: 0.587338  [12864/60000]\n",
      "loss: 0.572619  [19264/60000]\n",
      "loss: 0.657360  [25664/60000]\n",
      "loss: 0.570291  [32064/60000]\n",
      "loss: 0.443213  [38464/60000]\n",
      "loss: 0.582999  [44864/60000]\n",
      "loss: 0.543721  [51264/60000]\n",
      "loss: 0.587079  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.555350 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.598360  [   64/60000]\n",
      "loss: 0.434227  [ 6464/60000]\n",
      "loss: 0.741261  [12864/60000]\n",
      "loss: 0.516260  [19264/60000]\n",
      "loss: 0.470201  [25664/60000]\n",
      "loss: 0.577120  [32064/60000]\n",
      "loss: 0.541330  [38464/60000]\n",
      "loss: 0.599063  [44864/60000]\n",
      "loss: 0.495827  [51264/60000]\n",
      "loss: 0.387874  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.502477 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.525715  [   64/60000]\n",
      "loss: 0.577439  [ 6464/60000]\n",
      "loss: 0.779833  [12864/60000]\n",
      "loss: 0.527026  [19264/60000]\n",
      "loss: 0.409472  [25664/60000]\n",
      "loss: 0.464720  [32064/60000]\n",
      "loss: 0.563188  [38464/60000]\n",
      "loss: 0.443236  [44864/60000]\n",
      "loss: 0.498582  [51264/60000]\n",
      "loss: 0.657984  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.467356 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.404522  [   64/60000]\n",
      "loss: 0.460621  [ 6464/60000]\n",
      "loss: 0.472030  [12864/60000]\n",
      "loss: 0.498855  [19264/60000]\n",
      "loss: 0.564312  [25664/60000]\n",
      "loss: 0.409654  [32064/60000]\n",
      "loss: 0.384144  [38464/60000]\n",
      "loss: 0.524576  [44864/60000]\n",
      "loss: 0.469607  [51264/60000]\n",
      "loss: 0.353793  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.446180 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.232581  [   64/60000]\n",
      "loss: 0.507491  [ 6464/60000]\n",
      "loss: 0.411052  [12864/60000]\n",
      "loss: 0.449266  [19264/60000]\n",
      "loss: 0.430793  [25664/60000]\n",
      "loss: 0.314904  [32064/60000]\n",
      "loss: 0.421442  [38464/60000]\n",
      "loss: 0.411742  [44864/60000]\n",
      "loss: 0.313454  [51264/60000]\n",
      "loss: 0.438763  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.431814 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.377882  [   64/60000]\n",
      "loss: 0.372557  [ 6464/60000]\n",
      "loss: 0.292650  [12864/60000]\n",
      "loss: 0.544718  [19264/60000]\n",
      "loss: 0.415328  [25664/60000]\n",
      "loss: 0.303896  [32064/60000]\n",
      "loss: 0.513776  [38464/60000]\n",
      "loss: 0.371316  [44864/60000]\n",
      "loss: 0.311426  [51264/60000]\n",
      "loss: 0.437770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.417388 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.443606  [   64/60000]\n",
      "loss: 0.371098  [ 6464/60000]\n",
      "loss: 0.333560  [12864/60000]\n",
      "loss: 0.370001  [19264/60000]\n",
      "loss: 0.294315  [25664/60000]\n",
      "loss: 0.378581  [32064/60000]\n",
      "loss: 0.308674  [38464/60000]\n",
      "loss: 0.396044  [44864/60000]\n",
      "loss: 0.506576  [51264/60000]\n",
      "loss: 0.415719  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.406779 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.241633  [   64/60000]\n",
      "loss: 0.496020  [ 6464/60000]\n",
      "loss: 0.314502  [12864/60000]\n",
      "loss: 0.287573  [19264/60000]\n",
      "loss: 0.452535  [25664/60000]\n",
      "loss: 0.329275  [32064/60000]\n",
      "loss: 0.389190  [38464/60000]\n",
      "loss: 0.324320  [44864/60000]\n",
      "loss: 0.307831  [51264/60000]\n",
      "loss: 0.285548  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.396433 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.305324  [   64/60000]\n",
      "loss: 0.379134  [ 6464/60000]\n",
      "loss: 0.395347  [12864/60000]\n",
      "loss: 0.275944  [19264/60000]\n",
      "loss: 0.290748  [25664/60000]\n",
      "loss: 0.258791  [32064/60000]\n",
      "loss: 0.380261  [38464/60000]\n",
      "loss: 0.370836  [44864/60000]\n",
      "loss: 0.249217  [51264/60000]\n",
      "loss: 0.218041  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.390487 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.315960  [   64/60000]\n",
      "loss: 0.517228  [ 6464/60000]\n",
      "loss: 0.496776  [12864/60000]\n",
      "loss: 0.364210  [19264/60000]\n",
      "loss: 0.389186  [25664/60000]\n",
      "loss: 0.297334  [32064/60000]\n",
      "loss: 0.354399  [38464/60000]\n",
      "loss: 0.290029  [44864/60000]\n",
      "loss: 0.496398  [51264/60000]\n",
      "loss: 0.537664  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.395861 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.450289  [   64/60000]\n",
      "loss: 0.314749  [ 6464/60000]\n",
      "loss: 0.485011  [12864/60000]\n",
      "loss: 0.450582  [19264/60000]\n",
      "loss: 0.182745  [25664/60000]\n",
      "loss: 0.318793  [32064/60000]\n",
      "loss: 0.217659  [38464/60000]\n",
      "loss: 0.247744  [44864/60000]\n",
      "loss: 0.319695  [51264/60000]\n",
      "loss: 0.439742  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.341039 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.186760  [   64/60000]\n",
      "loss: 0.385376  [ 6464/60000]\n",
      "loss: 0.419423  [12864/60000]\n",
      "loss: 0.323496  [19264/60000]\n",
      "loss: 0.321074  [25664/60000]\n",
      "loss: 0.430598  [32064/60000]\n",
      "loss: 0.496483  [38464/60000]\n",
      "loss: 0.235958  [44864/60000]\n",
      "loss: 0.294539  [51264/60000]\n",
      "loss: 0.259052  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.330625 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.289046  [   64/60000]\n",
      "loss: 0.183869  [ 6464/60000]\n",
      "loss: 0.226938  [12864/60000]\n",
      "loss: 0.269330  [19264/60000]\n",
      "loss: 0.274696  [25664/60000]\n",
      "loss: 0.322366  [32064/60000]\n",
      "loss: 0.193135  [38464/60000]\n",
      "loss: 0.144246  [44864/60000]\n",
      "loss: 0.261766  [51264/60000]\n",
      "loss: 0.570998  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.352039 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.204293  [   64/60000]\n",
      "loss: 0.304212  [ 6464/60000]\n",
      "loss: 0.283858  [12864/60000]\n",
      "loss: 0.260670  [19264/60000]\n",
      "loss: 0.272756  [25664/60000]\n",
      "loss: 0.398329  [32064/60000]\n",
      "loss: 0.207398  [38464/60000]\n",
      "loss: 0.124821  [44864/60000]\n",
      "loss: 0.147435  [51264/60000]\n",
      "loss: 0.195873  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.333728 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.140682  [   64/60000]\n",
      "loss: 0.201501  [ 6464/60000]\n",
      "loss: 0.222557  [12864/60000]\n",
      "loss: 0.218892  [19264/60000]\n",
      "loss: 0.325508  [25664/60000]\n",
      "loss: 0.368808  [32064/60000]\n",
      "loss: 0.173828  [38464/60000]\n",
      "loss: 0.217029  [44864/60000]\n",
      "loss: 0.139475  [51264/60000]\n",
      "loss: 0.257618  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.314762 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.227402  [   64/60000]\n",
      "loss: 0.168168  [ 6464/60000]\n",
      "loss: 0.106282  [12864/60000]\n",
      "loss: 0.163322  [19264/60000]\n",
      "loss: 0.126163  [25664/60000]\n",
      "loss: 0.169097  [32064/60000]\n",
      "loss: 0.079892  [38464/60000]\n",
      "loss: 0.274067  [44864/60000]\n",
      "loss: 0.231674  [51264/60000]\n",
      "loss: 0.150520  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.342415 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.225318  [   64/60000]\n",
      "loss: 0.172276  [ 6464/60000]\n",
      "loss: 0.112870  [12864/60000]\n",
      "loss: 0.048834  [19264/60000]\n",
      "loss: 0.230243  [25664/60000]\n",
      "loss: 0.183756  [32064/60000]\n",
      "loss: 0.264035  [38464/60000]\n",
      "loss: 0.126646  [44864/60000]\n",
      "loss: 0.116595  [51264/60000]\n",
      "loss: 0.179845  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.324061 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.072719  [   64/60000]\n",
      "loss: 0.197953  [ 6464/60000]\n",
      "loss: 0.067370  [12864/60000]\n",
      "loss: 0.143819  [19264/60000]\n",
      "loss: 0.161119  [25664/60000]\n",
      "loss: 0.129689  [32064/60000]\n",
      "loss: 0.150052  [38464/60000]\n",
      "loss: 0.177844  [44864/60000]\n",
      "loss: 0.122855  [51264/60000]\n",
      "loss: 0.079697  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.322499 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.259658  [   64/60000]\n",
      "loss: 0.046938  [ 6464/60000]\n",
      "loss: 0.093294  [12864/60000]\n",
      "loss: 0.177753  [19264/60000]\n",
      "loss: 0.050951  [25664/60000]\n",
      "loss: 0.125512  [32064/60000]\n",
      "loss: 0.161881  [38464/60000]\n",
      "loss: 0.254060  [44864/60000]\n",
      "loss: 0.039507  [51264/60000]\n",
      "loss: 0.103959  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.332811 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.520377  [   64/60000]\n",
      "loss: 2.273412  [ 6464/60000]\n",
      "loss: 2.334095  [12864/60000]\n",
      "loss: 2.207384  [19264/60000]\n",
      "loss: 2.257616  [25664/60000]\n",
      "loss: 2.124066  [32064/60000]\n",
      "loss: 2.119332  [38464/60000]\n",
      "loss: 2.096593  [44864/60000]\n",
      "loss: 2.046721  [51264/60000]\n",
      "loss: 1.965843  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.985288 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.003755  [   64/60000]\n",
      "loss: 1.886981  [ 6464/60000]\n",
      "loss: 1.925348  [12864/60000]\n",
      "loss: 1.896730  [19264/60000]\n",
      "loss: 1.857167  [25664/60000]\n",
      "loss: 1.755218  [32064/60000]\n",
      "loss: 1.720965  [38464/60000]\n",
      "loss: 1.784566  [44864/60000]\n",
      "loss: 1.778831  [51264/60000]\n",
      "loss: 1.660704  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.677327 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.575435  [   64/60000]\n",
      "loss: 1.640248  [ 6464/60000]\n",
      "loss: 1.531810  [12864/60000]\n",
      "loss: 1.641903  [19264/60000]\n",
      "loss: 1.465783  [25664/60000]\n",
      "loss: 1.561151  [32064/60000]\n",
      "loss: 1.511641  [38464/60000]\n",
      "loss: 1.556968  [44864/60000]\n",
      "loss: 1.462891  [51264/60000]\n",
      "loss: 1.457556  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 1.472421 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.430372  [   64/60000]\n",
      "loss: 1.573500  [ 6464/60000]\n",
      "loss: 1.379554  [12864/60000]\n",
      "loss: 1.432816  [19264/60000]\n",
      "loss: 1.424076  [25664/60000]\n",
      "loss: 1.439814  [32064/60000]\n",
      "loss: 1.410505  [38464/60000]\n",
      "loss: 1.390753  [44864/60000]\n",
      "loss: 1.552915  [51264/60000]\n",
      "loss: 1.314150  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 1.315962 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.268851  [   64/60000]\n",
      "loss: 1.370202  [ 6464/60000]\n",
      "loss: 1.311907  [12864/60000]\n",
      "loss: 1.315649  [19264/60000]\n",
      "loss: 1.293056  [25664/60000]\n",
      "loss: 1.378210  [32064/60000]\n",
      "loss: 1.334594  [38464/60000]\n",
      "loss: 1.243098  [44864/60000]\n",
      "loss: 1.155107  [51264/60000]\n",
      "loss: 1.307845  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.203427 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.303472  [   64/60000]\n",
      "loss: 1.254303  [ 6464/60000]\n",
      "loss: 1.121755  [12864/60000]\n",
      "loss: 1.219357  [19264/60000]\n",
      "loss: 1.214893  [25664/60000]\n",
      "loss: 1.169686  [32064/60000]\n",
      "loss: 1.240378  [38464/60000]\n",
      "loss: 0.975192  [44864/60000]\n",
      "loss: 1.110162  [51264/60000]\n",
      "loss: 1.110560  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.117819 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.021245  [   64/60000]\n",
      "loss: 1.238886  [ 6464/60000]\n",
      "loss: 1.098000  [12864/60000]\n",
      "loss: 1.147826  [19264/60000]\n",
      "loss: 1.085594  [25664/60000]\n",
      "loss: 1.105507  [32064/60000]\n",
      "loss: 1.096350  [38464/60000]\n",
      "loss: 1.048103  [44864/60000]\n",
      "loss: 1.145470  [51264/60000]\n",
      "loss: 1.107239  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 1.052145 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.108718  [   64/60000]\n",
      "loss: 1.059265  [ 6464/60000]\n",
      "loss: 1.058129  [12864/60000]\n",
      "loss: 0.942740  [19264/60000]\n",
      "loss: 0.967317  [25664/60000]\n",
      "loss: 1.070914  [32064/60000]\n",
      "loss: 0.995293  [38464/60000]\n",
      "loss: 1.083513  [44864/60000]\n",
      "loss: 1.114725  [51264/60000]\n",
      "loss: 1.048196  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.004904 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.040601  [   64/60000]\n",
      "loss: 0.999179  [ 6464/60000]\n",
      "loss: 0.868453  [12864/60000]\n",
      "loss: 1.004203  [19264/60000]\n",
      "loss: 0.941566  [25664/60000]\n",
      "loss: 0.914618  [32064/60000]\n",
      "loss: 0.964457  [38464/60000]\n",
      "loss: 1.070421  [44864/60000]\n",
      "loss: 0.943577  [51264/60000]\n",
      "loss: 1.126116  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.4%, Avg loss: 0.962341 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.986082  [   64/60000]\n",
      "loss: 0.956176  [ 6464/60000]\n",
      "loss: 1.031193  [12864/60000]\n",
      "loss: 0.843997  [19264/60000]\n",
      "loss: 1.001875  [25664/60000]\n",
      "loss: 1.023252  [32064/60000]\n",
      "loss: 0.953529  [38464/60000]\n",
      "loss: 0.997361  [44864/60000]\n",
      "loss: 0.944104  [51264/60000]\n",
      "loss: 0.989651  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.925426 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.089535  [   64/60000]\n",
      "loss: -2.323865  [ 6464/60000]\n",
      "loss: -4.380870  [12864/60000]\n",
      "loss: -6.496493  [19264/60000]\n",
      "loss: -8.785840  [25664/60000]\n",
      "loss: -11.484894  [32064/60000]\n",
      "loss: -14.986683  [38464/60000]\n",
      "loss: -16.044069  [44864/60000]\n",
      "loss: -20.389416  [51264/60000]\n",
      "loss: -24.141499  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: -24.647760 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -24.612560  [   64/60000]\n",
      "loss: -27.804535  [ 6464/60000]\n",
      "loss: -27.922489  [12864/60000]\n",
      "loss: -33.644909  [19264/60000]\n",
      "loss: -39.141773  [25664/60000]\n",
      "loss: -44.336700  [32064/60000]\n",
      "loss: -44.796440  [38464/60000]\n",
      "loss: -52.749104  [44864/60000]\n",
      "loss: -55.050636  [51264/60000]\n",
      "loss: -60.196072  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: -63.899816 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -58.977245  [   64/60000]\n",
      "loss: -68.159355  [ 6464/60000]\n",
      "loss: -68.377380  [12864/60000]\n",
      "loss: -75.773560  [19264/60000]\n",
      "loss: -82.858292  [25664/60000]\n",
      "loss: -91.551857  [32064/60000]\n",
      "loss: -99.000916  [38464/60000]\n",
      "loss: -93.176102  [44864/60000]\n",
      "loss: -113.306572  [51264/60000]\n",
      "loss: -120.835907  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: -118.372814 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -124.434296  [   64/60000]\n",
      "loss: -129.382172  [ 6464/60000]\n",
      "loss: -134.194550  [12864/60000]\n",
      "loss: -157.789398  [19264/60000]\n",
      "loss: -145.892670  [25664/60000]\n",
      "loss: -162.583008  [32064/60000]\n",
      "loss: -181.251282  [38464/60000]\n",
      "loss: -205.401123  [44864/60000]\n",
      "loss: -198.051971  [51264/60000]\n",
      "loss: -208.961929  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: -218.254470 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -220.750702  [   64/60000]\n",
      "loss: -217.243134  [ 6464/60000]\n",
      "loss: -255.326019  [12864/60000]\n",
      "loss: -269.670776  [19264/60000]\n",
      "loss: -278.950317  [25664/60000]\n",
      "loss: -332.501587  [32064/60000]\n",
      "loss: -299.507812  [38464/60000]\n",
      "loss: -329.528320  [44864/60000]\n",
      "loss: -376.954712  [51264/60000]\n",
      "loss: -354.719727  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: -392.319515 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -393.491608  [   64/60000]\n",
      "loss: -406.954590  [ 6464/60000]\n",
      "loss: -397.343506  [12864/60000]\n",
      "loss: -431.670959  [19264/60000]\n",
      "loss: -501.958923  [25664/60000]\n",
      "loss: -566.485596  [32064/60000]\n",
      "loss: -578.111328  [38464/60000]\n",
      "loss: -591.906616  [44864/60000]\n",
      "loss: -609.342224  [51264/60000]\n",
      "loss: -671.381958  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: -730.020415 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -636.079590  [   64/60000]\n",
      "loss: -778.997925  [ 6464/60000]\n",
      "loss: -814.263000  [12864/60000]\n",
      "loss: -823.087280  [19264/60000]\n",
      "loss: -898.793518  [25664/60000]\n",
      "loss: -921.415710  [32064/60000]\n",
      "loss: -985.425293  [38464/60000]\n",
      "loss: -1089.267822  [44864/60000]\n",
      "loss: -1125.507812  [51264/60000]\n",
      "loss: -1234.348389  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: -1179.637696 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -1376.299927  [   64/60000]\n",
      "loss: -1200.610352  [ 6464/60000]\n",
      "loss: -1411.221680  [12864/60000]\n",
      "loss: -1391.248657  [19264/60000]\n",
      "loss: -1405.710449  [25664/60000]\n",
      "loss: -1638.882690  [32064/60000]\n",
      "loss: -1735.789673  [38464/60000]\n",
      "loss: -1782.581787  [44864/60000]\n",
      "loss: -1974.581909  [51264/60000]\n",
      "loss: -1971.580078  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Avg loss: -2157.537584 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -2172.375977  [   64/60000]\n",
      "loss: -2363.639893  [ 6464/60000]\n",
      "loss: -2382.512695  [12864/60000]\n",
      "loss: -2451.566162  [19264/60000]\n",
      "loss: -2650.890625  [25664/60000]\n",
      "loss: -2902.545654  [32064/60000]\n",
      "loss: -3039.735352  [38464/60000]\n",
      "loss: -3219.594727  [44864/60000]\n",
      "loss: -3235.750000  [51264/60000]\n",
      "loss: -3709.749512  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Avg loss: -3935.069647 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -3556.424316  [   64/60000]\n",
      "loss: -4102.507812  [ 6464/60000]\n",
      "loss: -4268.979492  [12864/60000]\n",
      "loss: -4529.426270  [19264/60000]\n",
      "loss: -4350.177734  [25664/60000]\n",
      "loss: -5368.429199  [32064/60000]\n",
      "loss: -5448.645508  [38464/60000]\n",
      "loss: -5233.059570  [44864/60000]\n",
      "loss: -5895.794922  [51264/60000]\n",
      "loss: -7147.497070  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: -6780.047329 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.104776  [   64/60000]\n",
      "loss: -6.340598  [ 6464/60000]\n",
      "loss: -9.658379  [12864/60000]\n",
      "loss: -12.596819  [19264/60000]\n",
      "loss: -15.755912  [25664/60000]\n",
      "loss: -18.475464  [32064/60000]\n",
      "loss: -21.659538  [38464/60000]\n",
      "loss: -24.895416  [44864/60000]\n",
      "loss: -28.015522  [51264/60000]\n",
      "loss: -32.247665  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: -33.600274 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -32.971004  [   64/60000]\n",
      "loss: -37.147503  [ 6464/60000]\n",
      "loss: -40.387257  [12864/60000]\n",
      "loss: -42.789864  [19264/60000]\n",
      "loss: -48.134701  [25664/60000]\n",
      "loss: -50.631584  [32064/60000]\n",
      "loss: -53.696339  [38464/60000]\n",
      "loss: -58.707664  [44864/60000]\n",
      "loss: -63.152771  [51264/60000]\n",
      "loss: -68.048050  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: -70.066326 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -70.814087  [   64/60000]\n",
      "loss: -73.604568  [ 6464/60000]\n",
      "loss: -78.644318  [12864/60000]\n",
      "loss: -82.940903  [19264/60000]\n",
      "loss: -87.748840  [25664/60000]\n",
      "loss: -92.202812  [32064/60000]\n",
      "loss: -96.996704  [38464/60000]\n",
      "loss: -102.356094  [44864/60000]\n",
      "loss: -108.947449  [51264/60000]\n",
      "loss: -113.005814  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: -114.667916 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -115.926270  [   64/60000]\n",
      "loss: -120.656548  [ 6464/60000]\n",
      "loss: -126.504990  [12864/60000]\n",
      "loss: -130.008072  [19264/60000]\n",
      "loss: -134.037445  [25664/60000]\n",
      "loss: -143.365509  [32064/60000]\n",
      "loss: -147.608917  [38464/60000]\n",
      "loss: -155.210709  [44864/60000]\n",
      "loss: -156.757355  [51264/60000]\n",
      "loss: -168.318359  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: -166.681189 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -170.740448  [   64/60000]\n",
      "loss: -178.366028  [ 6464/60000]\n",
      "loss: -182.898514  [12864/60000]\n",
      "loss: -189.726959  [19264/60000]\n",
      "loss: -193.146454  [25664/60000]\n",
      "loss: -203.755249  [32064/60000]\n",
      "loss: -210.836349  [38464/60000]\n",
      "loss: -216.268646  [44864/60000]\n",
      "loss: -223.579010  [51264/60000]\n",
      "loss: -228.750580  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: -234.170657 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -231.195221  [   64/60000]\n",
      "loss: -241.758636  [ 6464/60000]\n",
      "loss: -243.025024  [12864/60000]\n",
      "loss: -253.714478  [19264/60000]\n",
      "loss: -260.629944  [25664/60000]\n",
      "loss: -265.766724  [32064/60000]\n",
      "loss: -276.979095  [38464/60000]\n",
      "loss: -276.190643  [44864/60000]\n",
      "loss: -293.449280  [51264/60000]\n",
      "loss: -306.272522  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: -306.188507 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -309.422821  [   64/60000]\n",
      "loss: -299.690674  [ 6464/60000]\n",
      "loss: -322.235840  [12864/60000]\n",
      "loss: -326.224182  [19264/60000]\n",
      "loss: -337.800354  [25664/60000]\n",
      "loss: -346.579285  [32064/60000]\n",
      "loss: -351.125000  [38464/60000]\n",
      "loss: -364.087952  [44864/60000]\n",
      "loss: -372.027588  [51264/60000]\n",
      "loss: -376.992676  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.7%, Avg loss: -383.187285 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -383.471344  [   64/60000]\n",
      "loss: -398.283508  [ 6464/60000]\n",
      "loss: -403.665344  [12864/60000]\n",
      "loss: -410.431427  [19264/60000]\n",
      "loss: -419.542053  [25664/60000]\n",
      "loss: -434.053772  [32064/60000]\n",
      "loss: -429.725433  [38464/60000]\n",
      "loss: -454.296051  [44864/60000]\n",
      "loss: -466.807556  [51264/60000]\n",
      "loss: -463.197815  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: -466.443610 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -477.872559  [   64/60000]\n",
      "loss: -492.345428  [ 6464/60000]\n",
      "loss: -497.552795  [12864/60000]\n",
      "loss: -512.873901  [19264/60000]\n",
      "loss: -524.468628  [25664/60000]\n",
      "loss: -533.990234  [32064/60000]\n",
      "loss: -543.695984  [38464/60000]\n",
      "loss: -550.118835  [44864/60000]\n",
      "loss: -558.608826  [51264/60000]\n",
      "loss: -568.734619  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: -560.043503 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -582.769226  [   64/60000]\n",
      "loss: -588.275391  [ 6464/60000]\n",
      "loss: -606.317871  [12864/60000]\n",
      "loss: -612.110229  [19264/60000]\n",
      "loss: -623.453308  [25664/60000]\n",
      "loss: -631.907715  [32064/60000]\n",
      "loss: -648.677368  [38464/60000]\n",
      "loss: -660.703552  [44864/60000]\n",
      "loss: -670.981384  [51264/60000]\n",
      "loss: -679.751465  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: -696.895682 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: -0.095242  [   64/60000]\n",
      "loss: -0.215995  [ 6464/60000]\n",
      "loss: -0.241631  [12864/60000]\n",
      "loss: -0.291061  [19264/60000]\n",
      "loss: -0.306838  [25664/60000]\n",
      "loss: -0.358597  [32064/60000]\n",
      "loss: -0.382579  [38464/60000]\n",
      "loss: -0.504996  [44864/60000]\n",
      "loss: -0.487909  [51264/60000]\n",
      "loss: -0.506500  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.4%, Avg loss: -0.551201 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -0.518424  [   64/60000]\n",
      "loss: -0.673124  [ 6464/60000]\n",
      "loss: -0.606643  [12864/60000]\n",
      "loss: -0.684591  [19264/60000]\n",
      "loss: -0.602318  [25664/60000]\n",
      "loss: -0.831886  [32064/60000]\n",
      "loss: -0.803126  [38464/60000]\n",
      "loss: -0.919874  [44864/60000]\n",
      "loss: -0.940887  [51264/60000]\n",
      "loss: -0.941949  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: -0.977968 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -0.887446  [   64/60000]\n",
      "loss: -1.001023  [ 6464/60000]\n",
      "loss: -1.033454  [12864/60000]\n",
      "loss: -1.100687  [19264/60000]\n",
      "loss: -1.182285  [25664/60000]\n",
      "loss: -1.154492  [32064/60000]\n",
      "loss: -1.201214  [38464/60000]\n",
      "loss: -1.384567  [44864/60000]\n",
      "loss: -1.283207  [51264/60000]\n",
      "loss: -1.509478  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Avg loss: -1.396161 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -1.383682  [   64/60000]\n",
      "loss: -1.368347  [ 6464/60000]\n",
      "loss: -1.423084  [12864/60000]\n",
      "loss: -1.640791  [19264/60000]\n",
      "loss: -1.475419  [25664/60000]\n",
      "loss: -1.488203  [32064/60000]\n",
      "loss: -1.673264  [38464/60000]\n",
      "loss: -1.728060  [44864/60000]\n",
      "loss: -1.607836  [51264/60000]\n",
      "loss: -1.768890  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Avg loss: -1.754780 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -1.806230  [   64/60000]\n",
      "loss: -1.845855  [ 6464/60000]\n",
      "loss: -1.676104  [12864/60000]\n",
      "loss: -1.950184  [19264/60000]\n",
      "loss: -1.863564  [25664/60000]\n",
      "loss: -1.831688  [32064/60000]\n",
      "loss: -2.117499  [38464/60000]\n",
      "loss: -1.952798  [44864/60000]\n",
      "loss: -1.814118  [51264/60000]\n",
      "loss: -1.966204  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: -2.098176 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -2.176783  [   64/60000]\n",
      "loss: -2.189289  [ 6464/60000]\n",
      "loss: -2.128203  [12864/60000]\n",
      "loss: -2.263396  [19264/60000]\n",
      "loss: -2.032039  [25664/60000]\n",
      "loss: -2.422312  [32064/60000]\n",
      "loss: -2.091655  [38464/60000]\n",
      "loss: -2.411942  [44864/60000]\n",
      "loss: -2.324548  [51264/60000]\n",
      "loss: -2.145811  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: -2.406286 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -2.513966  [   64/60000]\n",
      "loss: -2.647041  [ 6464/60000]\n",
      "loss: -2.363144  [12864/60000]\n",
      "loss: -2.336175  [19264/60000]\n",
      "loss: -2.468885  [25664/60000]\n",
      "loss: -2.411000  [32064/60000]\n",
      "loss: -2.507808  [38464/60000]\n",
      "loss: -2.539283  [44864/60000]\n",
      "loss: -2.499503  [51264/60000]\n",
      "loss: -2.711861  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: -2.674797 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -2.736020  [   64/60000]\n",
      "loss: -2.861805  [ 6464/60000]\n",
      "loss: -2.695048  [12864/60000]\n",
      "loss: -2.642534  [19264/60000]\n",
      "loss: -2.888709  [25664/60000]\n",
      "loss: -2.773149  [32064/60000]\n",
      "loss: -2.887162  [38464/60000]\n",
      "loss: -2.930103  [44864/60000]\n",
      "loss: -2.778539  [51264/60000]\n",
      "loss: -2.927449  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: -2.939606 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -3.025994  [   64/60000]\n",
      "loss: -2.983686  [ 6464/60000]\n",
      "loss: -2.984146  [12864/60000]\n",
      "loss: -3.118557  [19264/60000]\n",
      "loss: -3.249419  [25664/60000]\n",
      "loss: -3.063161  [32064/60000]\n",
      "loss: -3.235127  [38464/60000]\n",
      "loss: -2.873917  [44864/60000]\n",
      "loss: -3.406619  [51264/60000]\n",
      "loss: -2.855880  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: -3.179751 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -3.401350  [   64/60000]\n",
      "loss: -3.180880  [ 6464/60000]\n",
      "loss: -3.095615  [12864/60000]\n",
      "loss: -3.440355  [19264/60000]\n",
      "loss: -3.191868  [25664/60000]\n",
      "loss: -3.400335  [32064/60000]\n",
      "loss: -3.254277  [38464/60000]\n",
      "loss: -3.614879  [44864/60000]\n",
      "loss: -3.372573  [51264/60000]\n",
      "loss: -3.466031  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: -3.447747 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: -0.101038  [   64/60000]\n",
      "loss: -2.075596  [ 6464/60000]\n",
      "loss: -4.177445  [12864/60000]\n",
      "loss: -6.841823  [19264/60000]\n",
      "loss: -9.491264  [25664/60000]\n",
      "loss: -12.017921  [32064/60000]\n",
      "loss: -13.946414  [38464/60000]\n",
      "loss: -18.257341  [44864/60000]\n",
      "loss: -21.023655  [51264/60000]\n",
      "loss: -21.748177  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: -24.439771 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -25.441589  [   64/60000]\n",
      "loss: -28.485401  [ 6464/60000]\n",
      "loss: -30.684017  [12864/60000]\n",
      "loss: -36.955856  [19264/60000]\n",
      "loss: -38.595963  [25664/60000]\n",
      "loss: -43.063076  [32064/60000]\n",
      "loss: -46.779953  [38464/60000]\n",
      "loss: -51.193596  [44864/60000]\n",
      "loss: -59.858719  [51264/60000]\n",
      "loss: -54.503609  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: -61.065358 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -63.088146  [   64/60000]\n",
      "loss: -69.577057  [ 6464/60000]\n",
      "loss: -74.654221  [12864/60000]\n",
      "loss: -75.525330  [19264/60000]\n",
      "loss: -83.908760  [25664/60000]\n",
      "loss: -89.089180  [32064/60000]\n",
      "loss: -87.931946  [38464/60000]\n",
      "loss: -113.316338  [44864/60000]\n",
      "loss: -113.449425  [51264/60000]\n",
      "loss: -115.485680  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: -117.708132 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -120.579071  [   64/60000]\n",
      "loss: -130.775406  [ 6464/60000]\n",
      "loss: -127.697861  [12864/60000]\n",
      "loss: -145.186401  [19264/60000]\n",
      "loss: -151.545502  [25664/60000]\n",
      "loss: -167.434265  [32064/60000]\n",
      "loss: -180.914520  [38464/60000]\n",
      "loss: -176.777008  [44864/60000]\n",
      "loss: -197.048615  [51264/60000]\n",
      "loss: -226.841980  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: -220.668817 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -225.645172  [   64/60000]\n",
      "loss: -213.778641  [ 6464/60000]\n",
      "loss: -239.576248  [12864/60000]\n",
      "loss: -245.695953  [19264/60000]\n",
      "loss: -281.470459  [25664/60000]\n",
      "loss: -300.674744  [32064/60000]\n",
      "loss: -327.142090  [38464/60000]\n",
      "loss: -319.009674  [44864/60000]\n",
      "loss: -367.156982  [51264/60000]\n",
      "loss: -379.252167  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: -403.378702 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -390.934753  [   64/60000]\n",
      "loss: -392.197296  [ 6464/60000]\n",
      "loss: -460.366699  [12864/60000]\n",
      "loss: -465.289795  [19264/60000]\n",
      "loss: -450.657959  [25664/60000]\n",
      "loss: -525.383911  [32064/60000]\n",
      "loss: -535.194275  [38464/60000]\n",
      "loss: -576.507629  [44864/60000]\n",
      "loss: -633.618286  [51264/60000]\n",
      "loss: -642.953979  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: -683.464887 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -653.488403  [   64/60000]\n",
      "loss: -650.613525  [ 6464/60000]\n",
      "loss: -660.766785  [12864/60000]\n",
      "loss: -761.144287  [19264/60000]\n",
      "loss: -850.180176  [25664/60000]\n",
      "loss: -917.891785  [32064/60000]\n",
      "loss: -1060.995728  [38464/60000]\n",
      "loss: -1108.376953  [44864/60000]\n",
      "loss: -1103.324585  [51264/60000]\n",
      "loss: -1136.071289  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: -1258.929457 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -1096.012451  [   64/60000]\n",
      "loss: -1277.758301  [ 6464/60000]\n",
      "loss: -1418.568848  [12864/60000]\n",
      "loss: -1355.906006  [19264/60000]\n",
      "loss: -1566.864136  [25664/60000]\n",
      "loss: -1603.583008  [32064/60000]\n",
      "loss: -1804.061768  [38464/60000]\n",
      "loss: -1829.953735  [44864/60000]\n",
      "loss: -1801.927612  [51264/60000]\n",
      "loss: -2141.019531  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Avg loss: -2097.884528 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -2158.791748  [   64/60000]\n",
      "loss: -2235.823242  [ 6464/60000]\n",
      "loss: -2542.038818  [12864/60000]\n",
      "loss: -2607.351807  [19264/60000]\n",
      "loss: -2747.507812  [25664/60000]\n",
      "loss: -2945.512207  [32064/60000]\n",
      "loss: -3262.202393  [38464/60000]\n",
      "loss: -3151.043701  [44864/60000]\n",
      "loss: -3400.265625  [51264/60000]\n",
      "loss: -3136.497070  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: -3694.452685 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -3617.017334  [   64/60000]\n",
      "loss: -3970.777100  [ 6464/60000]\n",
      "loss: -4299.788086  [12864/60000]\n",
      "loss: -4432.074707  [19264/60000]\n",
      "loss: -4950.322266  [25664/60000]\n",
      "loss: -4648.434570  [32064/60000]\n",
      "loss: -5127.985352  [38464/60000]\n",
      "loss: -5805.895996  [44864/60000]\n",
      "loss: -6256.146484  [51264/60000]\n",
      "loss: -6858.657227  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: -6421.854741 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: -0.040622  [   64/60000]\n",
      "loss: -6.320148  [ 6464/60000]\n",
      "loss: -10.149149  [12864/60000]\n",
      "loss: -12.004250  [19264/60000]\n",
      "loss: -15.194681  [25664/60000]\n",
      "loss: -18.301704  [32064/60000]\n",
      "loss: -21.577816  [38464/60000]\n",
      "loss: -24.239174  [44864/60000]\n",
      "loss: -26.449678  [51264/60000]\n",
      "loss: -31.351315  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: -33.846028 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -33.552895  [   64/60000]\n",
      "loss: -36.537560  [ 6464/60000]\n",
      "loss: -40.167271  [12864/60000]\n",
      "loss: -43.820404  [19264/60000]\n",
      "loss: -46.404724  [25664/60000]\n",
      "loss: -51.742485  [32064/60000]\n",
      "loss: -55.251740  [38464/60000]\n",
      "loss: -59.217014  [44864/60000]\n",
      "loss: -62.997375  [51264/60000]\n",
      "loss: -67.074852  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: -69.526879 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -69.547607  [   64/60000]\n",
      "loss: -73.011292  [ 6464/60000]\n",
      "loss: -76.870773  [12864/60000]\n",
      "loss: -80.786285  [19264/60000]\n",
      "loss: -86.337418  [25664/60000]\n",
      "loss: -93.788528  [32064/60000]\n",
      "loss: -97.296783  [38464/60000]\n",
      "loss: -102.679581  [44864/60000]\n",
      "loss: -107.291344  [51264/60000]\n",
      "loss: -113.256409  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: -112.460739 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -112.635727  [   64/60000]\n",
      "loss: -121.895958  [ 6464/60000]\n",
      "loss: -125.467056  [12864/60000]\n",
      "loss: -131.102203  [19264/60000]\n",
      "loss: -137.858032  [25664/60000]\n",
      "loss: -143.933350  [32064/60000]\n",
      "loss: -146.828094  [38464/60000]\n",
      "loss: -154.889801  [44864/60000]\n",
      "loss: -159.983261  [51264/60000]\n",
      "loss: -166.240509  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: -167.255833 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -166.515518  [   64/60000]\n",
      "loss: -174.740372  [ 6464/60000]\n",
      "loss: -179.254761  [12864/60000]\n",
      "loss: -188.041931  [19264/60000]\n",
      "loss: -196.634201  [25664/60000]\n",
      "loss: -203.405151  [32064/60000]\n",
      "loss: -210.676239  [38464/60000]\n",
      "loss: -213.775970  [44864/60000]\n",
      "loss: -223.664276  [51264/60000]\n",
      "loss: -226.913010  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: -233.723694 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -235.814911  [   64/60000]\n",
      "loss: -240.618958  [ 6464/60000]\n",
      "loss: -243.351639  [12864/60000]\n",
      "loss: -249.786484  [19264/60000]\n",
      "loss: -259.329498  [25664/60000]\n",
      "loss: -272.336792  [32064/60000]\n",
      "loss: -274.468475  [38464/60000]\n",
      "loss: -281.533417  [44864/60000]\n",
      "loss: -295.680481  [51264/60000]\n",
      "loss: -301.779358  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: -300.676162 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -303.712463  [   64/60000]\n",
      "loss: -313.073792  [ 6464/60000]\n",
      "loss: -322.743317  [12864/60000]\n",
      "loss: -329.732574  [19264/60000]\n",
      "loss: -336.514954  [25664/60000]\n",
      "loss: -347.214508  [32064/60000]\n",
      "loss: -353.878723  [38464/60000]\n",
      "loss: -366.244690  [44864/60000]\n",
      "loss: -374.199219  [51264/60000]\n",
      "loss: -380.846649  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: -392.019047 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -387.125916  [   64/60000]\n",
      "loss: -386.122528  [ 6464/60000]\n",
      "loss: -401.189789  [12864/60000]\n",
      "loss: -406.088074  [19264/60000]\n",
      "loss: -418.473267  [25664/60000]\n",
      "loss: -430.204010  [32064/60000]\n",
      "loss: -446.441315  [38464/60000]\n",
      "loss: -452.280884  [44864/60000]\n",
      "loss: -465.343567  [51264/60000]\n",
      "loss: -479.330383  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: -478.658134 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -465.282562  [   64/60000]\n",
      "loss: -490.379700  [ 6464/60000]\n",
      "loss: -496.359741  [12864/60000]\n",
      "loss: -511.416870  [19264/60000]\n",
      "loss: -519.453064  [25664/60000]\n",
      "loss: -528.500854  [32064/60000]\n",
      "loss: -542.389954  [38464/60000]\n",
      "loss: -556.240112  [44864/60000]\n",
      "loss: -564.054688  [51264/60000]\n",
      "loss: -557.707886  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.3%, Avg loss: -580.429740 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -582.229675  [   64/60000]\n",
      "loss: -581.313354  [ 6464/60000]\n",
      "loss: -591.308472  [12864/60000]\n",
      "loss: -615.142029  [19264/60000]\n",
      "loss: -622.080017  [25664/60000]\n",
      "loss: -641.843262  [32064/60000]\n",
      "loss: -637.493652  [38464/60000]\n",
      "loss: -650.459595  [44864/60000]\n",
      "loss: -659.541016  [51264/60000]\n",
      "loss: -676.257324  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: -677.211453 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.094131  [   64/60000]\n",
      "loss: 0.021750  [ 6464/60000]\n",
      "loss: -0.046814  [12864/60000]\n",
      "loss: -0.105242  [19264/60000]\n",
      "loss: -0.170172  [25664/60000]\n",
      "loss: -0.227526  [32064/60000]\n",
      "loss: -0.229862  [38464/60000]\n",
      "loss: -0.322661  [44864/60000]\n",
      "loss: -0.405279  [51264/60000]\n",
      "loss: -0.454493  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.0%, Avg loss: -0.408129 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -0.384831  [   64/60000]\n",
      "loss: -0.481083  [ 6464/60000]\n",
      "loss: -0.515867  [12864/60000]\n",
      "loss: -0.549982  [19264/60000]\n",
      "loss: -0.643159  [25664/60000]\n",
      "loss: -0.595524  [32064/60000]\n",
      "loss: -0.654096  [38464/60000]\n",
      "loss: -0.844147  [44864/60000]\n",
      "loss: -0.729190  [51264/60000]\n",
      "loss: -0.785491  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: -0.852337 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -0.821892  [   64/60000]\n",
      "loss: -0.988911  [ 6464/60000]\n",
      "loss: -0.931003  [12864/60000]\n",
      "loss: -0.936142  [19264/60000]\n",
      "loss: -1.000187  [25664/60000]\n",
      "loss: -1.053502  [32064/60000]\n",
      "loss: -1.130598  [38464/60000]\n",
      "loss: -1.208002  [44864/60000]\n",
      "loss: -1.221698  [51264/60000]\n",
      "loss: -1.253195  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: -1.246654 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -1.310520  [   64/60000]\n",
      "loss: -1.352565  [ 6464/60000]\n",
      "loss: -1.279426  [12864/60000]\n",
      "loss: -1.280054  [19264/60000]\n",
      "loss: -1.430700  [25664/60000]\n",
      "loss: -1.319030  [32064/60000]\n",
      "loss: -1.410451  [38464/60000]\n",
      "loss: -1.429103  [44864/60000]\n",
      "loss: -1.695376  [51264/60000]\n",
      "loss: -1.848985  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: -1.611760 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -1.485333  [   64/60000]\n",
      "loss: -1.629823  [ 6464/60000]\n",
      "loss: -1.708367  [12864/60000]\n",
      "loss: -1.562237  [19264/60000]\n",
      "loss: -1.774144  [25664/60000]\n",
      "loss: -1.675222  [32064/60000]\n",
      "loss: -1.890591  [38464/60000]\n",
      "loss: -1.973200  [44864/60000]\n",
      "loss: -1.942102  [51264/60000]\n",
      "loss: -1.966475  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: -1.947853 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -1.717402  [   64/60000]\n",
      "loss: -1.969108  [ 6464/60000]\n",
      "loss: -2.137721  [12864/60000]\n",
      "loss: -1.944567  [19264/60000]\n",
      "loss: -1.880437  [25664/60000]\n",
      "loss: -2.080874  [32064/60000]\n",
      "loss: -2.152305  [38464/60000]\n",
      "loss: -2.123393  [44864/60000]\n",
      "loss: -2.359796  [51264/60000]\n",
      "loss: -2.060565  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: -2.270542 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -2.044972  [   64/60000]\n",
      "loss: -2.265696  [ 6464/60000]\n",
      "loss: -2.420097  [12864/60000]\n",
      "loss: -2.135336  [19264/60000]\n",
      "loss: -2.261071  [25664/60000]\n",
      "loss: -2.674575  [32064/60000]\n",
      "loss: -2.390855  [38464/60000]\n",
      "loss: -2.137788  [44864/60000]\n",
      "loss: -2.317528  [51264/60000]\n",
      "loss: -2.396324  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: -2.554781 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -2.460910  [   64/60000]\n",
      "loss: -2.629849  [ 6464/60000]\n",
      "loss: -2.508629  [12864/60000]\n",
      "loss: -2.282473  [19264/60000]\n",
      "loss: -2.806752  [25664/60000]\n",
      "loss: -2.661988  [32064/60000]\n",
      "loss: -2.449862  [38464/60000]\n",
      "loss: -2.919738  [44864/60000]\n",
      "loss: -2.603670  [51264/60000]\n",
      "loss: -2.892647  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: -2.823321 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -3.056356  [   64/60000]\n",
      "loss: -2.763837  [ 6464/60000]\n",
      "loss: -2.878251  [12864/60000]\n",
      "loss: -2.910455  [19264/60000]\n",
      "loss: -2.920052  [25664/60000]\n",
      "loss: -2.850236  [32064/60000]\n",
      "loss: -3.150825  [38464/60000]\n",
      "loss: -3.242031  [44864/60000]\n",
      "loss: -3.188618  [51264/60000]\n",
      "loss: -3.127048  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: -3.094870 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -3.282636  [   64/60000]\n",
      "loss: -3.296160  [ 6464/60000]\n",
      "loss: -2.829474  [12864/60000]\n",
      "loss: -3.105128  [19264/60000]\n",
      "loss: -2.969097  [25664/60000]\n",
      "loss: -3.367024  [32064/60000]\n",
      "loss: -2.976392  [38464/60000]\n",
      "loss: -3.080625  [44864/60000]\n",
      "loss: -3.404603  [51264/60000]\n",
      "loss: -3.506160  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: -3.267192 \n",
      "\n",
      "The min losses loss_fn=NLLLoss2d(), optimizer=<function get_Adadelta at 0x17eef9b20> - min_loss=0.3328112724954915, acc_while_min_loss=89.13\n"
     ]
    }
   ],
   "source": [
    "acc_while_min_loss = 0\n",
    "min_loss = 100\n",
    "accurate_loss = None\n",
    "accurate_optimizer = None\n",
    "epochs = 10\n",
    "for loss_fn in losses:\n",
    "    for optimizer in optimizers:\n",
    "        model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n",
    "            model,\n",
    "            optimizer,\n",
    "            train_dataloader,\n",
    "            test_dataloader,\n",
    "        )\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train(train_dataloader, model, loss_fn, optimizer(model))\n",
    "            loss, acc = test(test_dataloader, model, loss_fn)\n",
    "        if loss < min_loss:\n",
    "            acc_while_min_loss = acc\n",
    "            min_loss = loss\n",
    "            accurate_loss = loss_fn\n",
    "            accurate_optimizer = optimizer\n",
    "            accurate_model = model\n",
    "\n",
    "print(f\"The min losses {loss_fn=}, {optimizer=} - {min_loss=}, {acc_while_min_loss=}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a2f7c",
   "metadata": {},
   "source": [
    "Осталось сохранить модель для дальнейшего использования. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1ec53cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(accurate_model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674ae5e",
   "metadata": {},
   "source": [
    "Какая точность получилась? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5de00f",
   "metadata": {},
   "source": [
    "Задание: Перепишите датасет FashionMNISTDataset так, чтобы он возвращал вектор вместо матрицы и мою модель без слоя MyFlatten(). Запустите обучение заново. Какая точность получилась теперь?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e520d133-ba62-4c6f-be8d-07772cd326d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, file_dir, train=True, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        image_file = f\"{'train' if self.train else 't10k'}-images-idx3-ubyte\"\n",
    "        self.data = read_image_file(os.path.join(file_dir, image_file))\n",
    "\n",
    "        label_file = f\"{'train' if self.train else 't10k'}-labels-idx1-ubyte\"\n",
    "        self.targets = read_label_file(os.path.join(file_dir, label_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image.flatten(0, -1).float(), label\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a0b34b7d-7e33-41b8-b8bf-ba671fd4375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNISTDataset(\n",
    "    file_dir=\"data/FashionMNIST/raw\",\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "test_data = FashionMNISTDataset(\n",
    "    file_dir=\"data/FashionMNIST/raw\",\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01d93f2e-15eb-431e-b271-ad6b0007b45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.319198  [   64/60000]\n",
      "loss: 1.519175  [ 6464/60000]\n",
      "loss: 1.122489  [12864/60000]\n",
      "loss: 0.829344  [19264/60000]\n",
      "loss: 0.900774  [25664/60000]\n",
      "loss: 0.884610  [32064/60000]\n",
      "loss: 0.880130  [38464/60000]\n",
      "loss: 0.762805  [44864/60000]\n",
      "loss: 0.579842  [51264/60000]\n",
      "loss: 0.580557  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.670259 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.789046  [   64/60000]\n",
      "loss: 0.860897  [ 6464/60000]\n",
      "loss: 0.747140  [12864/60000]\n",
      "loss: 0.636272  [19264/60000]\n",
      "loss: 0.603183  [25664/60000]\n",
      "loss: 0.580076  [32064/60000]\n",
      "loss: 0.765607  [38464/60000]\n",
      "loss: 0.733366  [44864/60000]\n",
      "loss: 0.411866  [51264/60000]\n",
      "loss: 0.390146  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.554433 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.562130  [   64/60000]\n",
      "loss: 0.585519  [ 6464/60000]\n",
      "loss: 0.604226  [12864/60000]\n",
      "loss: 0.555615  [19264/60000]\n",
      "loss: 0.495642  [25664/60000]\n",
      "loss: 0.525236  [32064/60000]\n",
      "loss: 0.537397  [38464/60000]\n",
      "loss: 0.598412  [44864/60000]\n",
      "loss: 0.734854  [51264/60000]\n",
      "loss: 0.452130  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.499648 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.415722  [   64/60000]\n",
      "loss: 0.470929  [ 6464/60000]\n",
      "loss: 0.645894  [12864/60000]\n",
      "loss: 0.561150  [19264/60000]\n",
      "loss: 0.429623  [25664/60000]\n",
      "loss: 0.336737  [32064/60000]\n",
      "loss: 0.396632  [38464/60000]\n",
      "loss: 0.586273  [44864/60000]\n",
      "loss: 0.578897  [51264/60000]\n",
      "loss: 0.555471  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.474285 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.589135  [   64/60000]\n",
      "loss: 0.371755  [ 6464/60000]\n",
      "loss: 0.350489  [12864/60000]\n",
      "loss: 0.423247  [19264/60000]\n",
      "loss: 0.558882  [25664/60000]\n",
      "loss: 0.489129  [32064/60000]\n",
      "loss: 0.410215  [38464/60000]\n",
      "loss: 0.463126  [44864/60000]\n",
      "loss: 0.579066  [51264/60000]\n",
      "loss: 0.577774  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.448743 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.480584  [   64/60000]\n",
      "loss: 0.276788  [ 6464/60000]\n",
      "loss: 0.381390  [12864/60000]\n",
      "loss: 0.423919  [19264/60000]\n",
      "loss: 0.463101  [25664/60000]\n",
      "loss: 0.538921  [32064/60000]\n",
      "loss: 0.372298  [38464/60000]\n",
      "loss: 0.462079  [44864/60000]\n",
      "loss: 0.427073  [51264/60000]\n",
      "loss: 0.607984  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.434210 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.318017  [   64/60000]\n",
      "loss: 0.297906  [ 6464/60000]\n",
      "loss: 0.300482  [12864/60000]\n",
      "loss: 0.306899  [19264/60000]\n",
      "loss: 0.365993  [25664/60000]\n",
      "loss: 0.305300  [32064/60000]\n",
      "loss: 0.413115  [38464/60000]\n",
      "loss: 0.432909  [44864/60000]\n",
      "loss: 0.488850  [51264/60000]\n",
      "loss: 0.388731  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.419237 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.447839  [   64/60000]\n",
      "loss: 0.652320  [ 6464/60000]\n",
      "loss: 0.302049  [12864/60000]\n",
      "loss: 0.251106  [19264/60000]\n",
      "loss: 0.297534  [25664/60000]\n",
      "loss: 0.553294  [32064/60000]\n",
      "loss: 0.558858  [38464/60000]\n",
      "loss: 0.469244  [44864/60000]\n",
      "loss: 0.547562  [51264/60000]\n",
      "loss: 0.376847  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.411552 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.298428  [   64/60000]\n",
      "loss: 0.360859  [ 6464/60000]\n",
      "loss: 0.433049  [12864/60000]\n",
      "loss: 0.414748  [19264/60000]\n",
      "loss: 0.436676  [25664/60000]\n",
      "loss: 0.328388  [32064/60000]\n",
      "loss: 0.362274  [38464/60000]\n",
      "loss: 0.418973  [44864/60000]\n",
      "loss: 0.417445  [51264/60000]\n",
      "loss: 0.372502  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.400884 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.460836  [   64/60000]\n",
      "loss: 0.330061  [ 6464/60000]\n",
      "loss: 0.318643  [12864/60000]\n",
      "loss: 0.350329  [19264/60000]\n",
      "loss: 0.270345  [25664/60000]\n",
      "loss: 0.274158  [32064/60000]\n",
      "loss: 0.404151  [38464/60000]\n",
      "loss: 0.279255  [44864/60000]\n",
      "loss: 0.344933  [51264/60000]\n",
      "loss: 0.386390  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.394275 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.391750  [   64/60000]\n",
      "loss: 0.630857  [ 6464/60000]\n",
      "loss: 0.607533  [12864/60000]\n",
      "loss: 0.467083  [19264/60000]\n",
      "loss: 0.489404  [25664/60000]\n",
      "loss: 0.289506  [32064/60000]\n",
      "loss: 0.364408  [38464/60000]\n",
      "loss: 0.342073  [44864/60000]\n",
      "loss: 0.302596  [51264/60000]\n",
      "loss: 0.316405  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.382513 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.182492  [   64/60000]\n",
      "loss: 0.599091  [ 6464/60000]\n",
      "loss: 0.272554  [12864/60000]\n",
      "loss: 0.281435  [19264/60000]\n",
      "loss: 0.380926  [25664/60000]\n",
      "loss: 0.465603  [32064/60000]\n",
      "loss: 0.401196  [38464/60000]\n",
      "loss: 0.216528  [44864/60000]\n",
      "loss: 0.359522  [51264/60000]\n",
      "loss: 0.261730  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.351373 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.217446  [   64/60000]\n",
      "loss: 0.236118  [ 6464/60000]\n",
      "loss: 0.191766  [12864/60000]\n",
      "loss: 0.371645  [19264/60000]\n",
      "loss: 0.135072  [25664/60000]\n",
      "loss: 0.395074  [32064/60000]\n",
      "loss: 0.297953  [38464/60000]\n",
      "loss: 0.387540  [44864/60000]\n",
      "loss: 0.285514  [51264/60000]\n",
      "loss: 0.368161  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.349189 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.329858  [   64/60000]\n",
      "loss: 0.186755  [ 6464/60000]\n",
      "loss: 0.210748  [12864/60000]\n",
      "loss: 0.226033  [19264/60000]\n",
      "loss: 0.283922  [25664/60000]\n",
      "loss: 0.333588  [32064/60000]\n",
      "loss: 0.261402  [38464/60000]\n",
      "loss: 0.146002  [44864/60000]\n",
      "loss: 0.215274  [51264/60000]\n",
      "loss: 0.164103  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.329086 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.253058  [   64/60000]\n",
      "loss: 0.162325  [ 6464/60000]\n",
      "loss: 0.202109  [12864/60000]\n",
      "loss: 0.212328  [19264/60000]\n",
      "loss: 0.241635  [25664/60000]\n",
      "loss: 0.198131  [32064/60000]\n",
      "loss: 0.312997  [38464/60000]\n",
      "loss: 0.188166  [44864/60000]\n",
      "loss: 0.274352  [51264/60000]\n",
      "loss: 0.323663  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.336865 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.214185  [   64/60000]\n",
      "loss: 0.206503  [ 6464/60000]\n",
      "loss: 0.285308  [12864/60000]\n",
      "loss: 0.130035  [19264/60000]\n",
      "loss: 0.126282  [25664/60000]\n",
      "loss: 0.203394  [32064/60000]\n",
      "loss: 0.122418  [38464/60000]\n",
      "loss: 0.186823  [44864/60000]\n",
      "loss: 0.290072  [51264/60000]\n",
      "loss: 0.130165  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.308219 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.052206  [   64/60000]\n",
      "loss: 0.249598  [ 6464/60000]\n",
      "loss: 0.168820  [12864/60000]\n",
      "loss: 0.164603  [19264/60000]\n",
      "loss: 0.151167  [25664/60000]\n",
      "loss: 0.160806  [32064/60000]\n",
      "loss: 0.281354  [38464/60000]\n",
      "loss: 0.222607  [44864/60000]\n",
      "loss: 0.174993  [51264/60000]\n",
      "loss: 0.243943  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.313461 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.259140  [   64/60000]\n",
      "loss: 0.210013  [ 6464/60000]\n",
      "loss: 0.107555  [12864/60000]\n",
      "loss: 0.136851  [19264/60000]\n",
      "loss: 0.221128  [25664/60000]\n",
      "loss: 0.231931  [32064/60000]\n",
      "loss: 0.182806  [38464/60000]\n",
      "loss: 0.088072  [44864/60000]\n",
      "loss: 0.092700  [51264/60000]\n",
      "loss: 0.132631  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.323226 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.129822  [   64/60000]\n",
      "loss: 0.134835  [ 6464/60000]\n",
      "loss: 0.123304  [12864/60000]\n",
      "loss: 0.072172  [19264/60000]\n",
      "loss: 0.105439  [25664/60000]\n",
      "loss: 0.123532  [32064/60000]\n",
      "loss: 0.108714  [38464/60000]\n",
      "loss: 0.079082  [44864/60000]\n",
      "loss: 0.089387  [51264/60000]\n",
      "loss: 0.094996  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.306239 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.175184  [   64/60000]\n",
      "loss: 0.200354  [ 6464/60000]\n",
      "loss: 0.129026  [12864/60000]\n",
      "loss: 0.335019  [19264/60000]\n",
      "loss: 0.154856  [25664/60000]\n",
      "loss: 0.154569  [32064/60000]\n",
      "loss: 0.056810  [38464/60000]\n",
      "loss: 0.136948  [44864/60000]\n",
      "loss: 0.161367  [51264/60000]\n",
      "loss: 0.223271  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.353891 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.396426  [   64/60000]\n",
      "loss: 2.294644  [ 6464/60000]\n",
      "loss: 2.258567  [12864/60000]\n",
      "loss: 2.214738  [19264/60000]\n",
      "loss: 2.197820  [25664/60000]\n",
      "loss: 2.075523  [32064/60000]\n",
      "loss: 2.065864  [38464/60000]\n",
      "loss: 2.002101  [44864/60000]\n",
      "loss: 2.027547  [51264/60000]\n",
      "loss: 1.936759  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.6%, Avg loss: 1.964170 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.938020  [   64/60000]\n",
      "loss: 1.973693  [ 6464/60000]\n",
      "loss: 1.793617  [12864/60000]\n",
      "loss: 1.840460  [19264/60000]\n",
      "loss: 1.877388  [25664/60000]\n",
      "loss: 1.793646  [32064/60000]\n",
      "loss: 1.721506  [38464/60000]\n",
      "loss: 1.703046  [44864/60000]\n",
      "loss: 1.832873  [51264/60000]\n",
      "loss: 1.638989  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 1.666073 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.628462  [   64/60000]\n",
      "loss: 1.662718  [ 6464/60000]\n",
      "loss: 1.634017  [12864/60000]\n",
      "loss: 1.580612  [19264/60000]\n",
      "loss: 1.700033  [25664/60000]\n",
      "loss: 1.506109  [32064/60000]\n",
      "loss: 1.576409  [38464/60000]\n",
      "loss: 1.604388  [44864/60000]\n",
      "loss: 1.442357  [51264/60000]\n",
      "loss: 1.483296  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 1.462089 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.632365  [   64/60000]\n",
      "loss: 1.371721  [ 6464/60000]\n",
      "loss: 1.295639  [12864/60000]\n",
      "loss: 1.377926  [19264/60000]\n",
      "loss: 1.378617  [25664/60000]\n",
      "loss: 1.344538  [32064/60000]\n",
      "loss: 1.261998  [38464/60000]\n",
      "loss: 1.368445  [44864/60000]\n",
      "loss: 1.407708  [51264/60000]\n",
      "loss: 1.320068  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.306564 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.443613  [   64/60000]\n",
      "loss: 1.338847  [ 6464/60000]\n",
      "loss: 1.203786  [12864/60000]\n",
      "loss: 1.193397  [19264/60000]\n",
      "loss: 1.239608  [25664/60000]\n",
      "loss: 1.169431  [32064/60000]\n",
      "loss: 1.269471  [38464/60000]\n",
      "loss: 1.311335  [44864/60000]\n",
      "loss: 1.163485  [51264/60000]\n",
      "loss: 1.212502  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 1.195956 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.195417  [   64/60000]\n",
      "loss: 1.193417  [ 6464/60000]\n",
      "loss: 1.293704  [12864/60000]\n",
      "loss: 1.230699  [19264/60000]\n",
      "loss: 1.164598  [25664/60000]\n",
      "loss: 1.198157  [32064/60000]\n",
      "loss: 1.129082  [38464/60000]\n",
      "loss: 1.208343  [44864/60000]\n",
      "loss: 1.167489  [51264/60000]\n",
      "loss: 1.098392  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.116827 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.082996  [   64/60000]\n",
      "loss: 1.034975  [ 6464/60000]\n",
      "loss: 1.135093  [12864/60000]\n",
      "loss: 1.194951  [19264/60000]\n",
      "loss: 1.171931  [25664/60000]\n",
      "loss: 1.226763  [32064/60000]\n",
      "loss: 0.994983  [38464/60000]\n",
      "loss: 1.012607  [44864/60000]\n",
      "loss: 0.883116  [51264/60000]\n",
      "loss: 0.980072  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.043211 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.032703  [   64/60000]\n",
      "loss: 1.161623  [ 6464/60000]\n",
      "loss: 0.949354  [12864/60000]\n",
      "loss: 1.038081  [19264/60000]\n",
      "loss: 1.077691  [25664/60000]\n",
      "loss: 1.129947  [32064/60000]\n",
      "loss: 1.103811  [38464/60000]\n",
      "loss: 1.107252  [44864/60000]\n",
      "loss: 0.995330  [51264/60000]\n",
      "loss: 0.970465  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.998756 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.938310  [   64/60000]\n",
      "loss: 0.997274  [ 6464/60000]\n",
      "loss: 0.976820  [12864/60000]\n",
      "loss: 0.911414  [19264/60000]\n",
      "loss: 1.037449  [25664/60000]\n",
      "loss: 0.982088  [32064/60000]\n",
      "loss: 1.113062  [38464/60000]\n",
      "loss: 1.266780  [44864/60000]\n",
      "loss: 0.869776  [51264/60000]\n",
      "loss: 0.866776  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.949202 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.031965  [   64/60000]\n",
      "loss: 1.103303  [ 6464/60000]\n",
      "loss: 1.009595  [12864/60000]\n",
      "loss: 0.858379  [19264/60000]\n",
      "loss: 0.902742  [25664/60000]\n",
      "loss: 0.873233  [32064/60000]\n",
      "loss: 1.054687  [38464/60000]\n",
      "loss: 0.937940  [44864/60000]\n",
      "loss: 0.802664  [51264/60000]\n",
      "loss: 0.797743  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.912388 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: -0.089869  [   64/60000]\n",
      "loss: -2.362972  [ 6464/60000]\n",
      "loss: -4.740993  [12864/60000]\n",
      "loss: -6.928323  [19264/60000]\n",
      "loss: -8.798435  [25664/60000]\n",
      "loss: -10.855560  [32064/60000]\n",
      "loss: -14.744396  [38464/60000]\n",
      "loss: -17.563957  [44864/60000]\n",
      "loss: -20.909439  [51264/60000]\n",
      "loss: -23.566364  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: -24.807794 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -24.611855  [   64/60000]\n",
      "loss: -25.052559  [ 6464/60000]\n",
      "loss: -33.484135  [12864/60000]\n",
      "loss: -38.669479  [19264/60000]\n",
      "loss: -34.525543  [25664/60000]\n",
      "loss: -44.208176  [32064/60000]\n",
      "loss: -49.417786  [38464/60000]\n",
      "loss: -49.824814  [44864/60000]\n",
      "loss: -52.250652  [51264/60000]\n",
      "loss: -63.961349  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: -63.328802 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -55.250107  [   64/60000]\n",
      "loss: -64.558182  [ 6464/60000]\n",
      "loss: -70.202003  [12864/60000]\n",
      "loss: -72.991562  [19264/60000]\n",
      "loss: -83.975708  [25664/60000]\n",
      "loss: -86.299362  [32064/60000]\n",
      "loss: -98.264580  [38464/60000]\n",
      "loss: -96.351608  [44864/60000]\n",
      "loss: -106.403854  [51264/60000]\n",
      "loss: -103.492325  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: -121.126219 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -129.240845  [   64/60000]\n",
      "loss: -121.679939  [ 6464/60000]\n",
      "loss: -134.595169  [12864/60000]\n",
      "loss: -154.540161  [19264/60000]\n",
      "loss: -156.217926  [25664/60000]\n",
      "loss: -162.851944  [32064/60000]\n",
      "loss: -172.652100  [38464/60000]\n",
      "loss: -204.090103  [44864/60000]\n",
      "loss: -199.701324  [51264/60000]\n",
      "loss: -197.517822  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: -219.619184 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -212.880219  [   64/60000]\n",
      "loss: -241.037338  [ 6464/60000]\n",
      "loss: -241.923416  [12864/60000]\n",
      "loss: -257.534393  [19264/60000]\n",
      "loss: -263.697815  [25664/60000]\n",
      "loss: -292.515381  [32064/60000]\n",
      "loss: -297.450256  [38464/60000]\n",
      "loss: -327.490723  [44864/60000]\n",
      "loss: -370.786926  [51264/60000]\n",
      "loss: -357.363495  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: -386.595977 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -396.489319  [   64/60000]\n",
      "loss: -402.279449  [ 6464/60000]\n",
      "loss: -453.808228  [12864/60000]\n",
      "loss: -455.375732  [19264/60000]\n",
      "loss: -507.886993  [25664/60000]\n",
      "loss: -542.241089  [32064/60000]\n",
      "loss: -458.294189  [38464/60000]\n",
      "loss: -590.927002  [44864/60000]\n",
      "loss: -604.442444  [51264/60000]\n",
      "loss: -651.993347  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: -667.770210 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -695.781494  [   64/60000]\n",
      "loss: -780.275146  [ 6464/60000]\n",
      "loss: -798.028809  [12864/60000]\n",
      "loss: -810.821716  [19264/60000]\n",
      "loss: -835.314453  [25664/60000]\n",
      "loss: -883.195435  [32064/60000]\n",
      "loss: -922.907104  [38464/60000]\n",
      "loss: -1106.399658  [44864/60000]\n",
      "loss: -1040.898682  [51264/60000]\n",
      "loss: -1187.535156  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: -1195.970302 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -1155.853027  [   64/60000]\n",
      "loss: -1287.403198  [ 6464/60000]\n",
      "loss: -1374.649536  [12864/60000]\n",
      "loss: -1454.109985  [19264/60000]\n",
      "loss: -1636.999512  [25664/60000]\n",
      "loss: -1510.050049  [32064/60000]\n",
      "loss: -1557.140137  [38464/60000]\n",
      "loss: -1899.730713  [44864/60000]\n",
      "loss: -1976.839844  [51264/60000]\n",
      "loss: -1872.626709  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Avg loss: -2142.583400 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -1942.090332  [   64/60000]\n",
      "loss: -2347.076660  [ 6464/60000]\n",
      "loss: -2295.886475  [12864/60000]\n",
      "loss: -2760.161133  [19264/60000]\n",
      "loss: -2732.697266  [25664/60000]\n",
      "loss: -2770.989746  [32064/60000]\n",
      "loss: -3312.168701  [38464/60000]\n",
      "loss: -3239.802002  [44864/60000]\n",
      "loss: -3471.051758  [51264/60000]\n",
      "loss: -3772.075684  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: -3785.806449 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -3761.589600  [   64/60000]\n",
      "loss: -3811.230957  [ 6464/60000]\n",
      "loss: -4197.226562  [12864/60000]\n",
      "loss: -4733.672852  [19264/60000]\n",
      "loss: -4910.704102  [25664/60000]\n",
      "loss: -4779.994141  [32064/60000]\n",
      "loss: -5505.997070  [38464/60000]\n",
      "loss: -6081.912109  [44864/60000]\n",
      "loss: -5518.292969  [51264/60000]\n",
      "loss: -6287.167480  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: -6526.724364 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: -0.124924  [   64/60000]\n",
      "loss: -6.543362  [ 6464/60000]\n",
      "loss: -9.799276  [12864/60000]\n",
      "loss: -12.731817  [19264/60000]\n",
      "loss: -15.875926  [25664/60000]\n",
      "loss: -19.112362  [32064/60000]\n",
      "loss: -21.355223  [38464/60000]\n",
      "loss: -24.925224  [44864/60000]\n",
      "loss: -28.299904  [51264/60000]\n",
      "loss: -32.210640  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: -33.885902 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -33.228394  [   64/60000]\n",
      "loss: -35.246193  [ 6464/60000]\n",
      "loss: -40.629612  [12864/60000]\n",
      "loss: -43.893852  [19264/60000]\n",
      "loss: -48.231491  [25664/60000]\n",
      "loss: -52.111801  [32064/60000]\n",
      "loss: -55.643307  [38464/60000]\n",
      "loss: -58.422459  [44864/60000]\n",
      "loss: -65.183868  [51264/60000]\n",
      "loss: -66.497826  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: -69.328786 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -69.226868  [   64/60000]\n",
      "loss: -74.931671  [ 6464/60000]\n",
      "loss: -79.015106  [12864/60000]\n",
      "loss: -82.068008  [19264/60000]\n",
      "loss: -86.769806  [25664/60000]\n",
      "loss: -92.288429  [32064/60000]\n",
      "loss: -97.711739  [38464/60000]\n",
      "loss: -101.398277  [44864/60000]\n",
      "loss: -106.965096  [51264/60000]\n",
      "loss: -110.834305  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: -116.273819 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -110.538147  [   64/60000]\n",
      "loss: -119.975082  [ 6464/60000]\n",
      "loss: -119.672363  [12864/60000]\n",
      "loss: -131.842407  [19264/60000]\n",
      "loss: -134.809998  [25664/60000]\n",
      "loss: -144.472015  [32064/60000]\n",
      "loss: -149.432648  [38464/60000]\n",
      "loss: -155.440903  [44864/60000]\n",
      "loss: -160.169128  [51264/60000]\n",
      "loss: -166.625061  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: -167.308200 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -168.409210  [   64/60000]\n",
      "loss: -175.324341  [ 6464/60000]\n",
      "loss: -178.867188  [12864/60000]\n",
      "loss: -187.435837  [19264/60000]\n",
      "loss: -189.183868  [25664/60000]\n",
      "loss: -200.417603  [32064/60000]\n",
      "loss: -209.241516  [38464/60000]\n",
      "loss: -219.339325  [44864/60000]\n",
      "loss: -222.987976  [51264/60000]\n",
      "loss: -219.443863  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: -231.113381 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -234.931030  [   64/60000]\n",
      "loss: -242.064423  [ 6464/60000]\n",
      "loss: -248.255081  [12864/60000]\n",
      "loss: -248.995209  [19264/60000]\n",
      "loss: -259.596832  [25664/60000]\n",
      "loss: -266.482178  [32064/60000]\n",
      "loss: -277.607086  [38464/60000]\n",
      "loss: -286.084381  [44864/60000]\n",
      "loss: -294.700684  [51264/60000]\n",
      "loss: -303.565186  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: -309.575642 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -300.305267  [   64/60000]\n",
      "loss: -314.615234  [ 6464/60000]\n",
      "loss: -323.145264  [12864/60000]\n",
      "loss: -327.383484  [19264/60000]\n",
      "loss: -340.519470  [25664/60000]\n",
      "loss: -345.178741  [32064/60000]\n",
      "loss: -351.746613  [38464/60000]\n",
      "loss: -362.285828  [44864/60000]\n",
      "loss: -378.323486  [51264/60000]\n",
      "loss: -382.627686  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: -387.708576 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -386.648285  [   64/60000]\n",
      "loss: -401.075470  [ 6464/60000]\n",
      "loss: -408.272888  [12864/60000]\n",
      "loss: -415.786499  [19264/60000]\n",
      "loss: -426.241943  [25664/60000]\n",
      "loss: -430.818268  [32064/60000]\n",
      "loss: -438.697266  [38464/60000]\n",
      "loss: -453.525330  [44864/60000]\n",
      "loss: -463.788757  [51264/60000]\n",
      "loss: -465.610840  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: -463.831409 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -481.302490  [   64/60000]\n",
      "loss: -487.418976  [ 6464/60000]\n",
      "loss: -496.408508  [12864/60000]\n",
      "loss: -509.753204  [19264/60000]\n",
      "loss: -514.952881  [25664/60000]\n",
      "loss: -520.055664  [32064/60000]\n",
      "loss: -535.001892  [38464/60000]\n",
      "loss: -544.688721  [44864/60000]\n",
      "loss: -562.226929  [51264/60000]\n",
      "loss: -571.726685  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: -567.735663 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -581.543457  [   64/60000]\n",
      "loss: -579.606323  [ 6464/60000]\n",
      "loss: -603.556641  [12864/60000]\n",
      "loss: -614.587524  [19264/60000]\n",
      "loss: -621.327148  [25664/60000]\n",
      "loss: -631.805176  [32064/60000]\n",
      "loss: -647.500488  [38464/60000]\n",
      "loss: -661.933716  [44864/60000]\n",
      "loss: -671.290161  [51264/60000]\n",
      "loss: -685.130249  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: -686.225178 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.135711  [   64/60000]\n",
      "loss: 0.077663  [ 6464/60000]\n",
      "loss: -0.080390  [12864/60000]\n",
      "loss: 0.062374  [19264/60000]\n",
      "loss: -0.071965  [25664/60000]\n",
      "loss: -0.155500  [32064/60000]\n",
      "loss: -0.232354  [38464/60000]\n",
      "loss: -0.278361  [44864/60000]\n",
      "loss: -0.355102  [51264/60000]\n",
      "loss: -0.377901  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 31.7%, Avg loss: -0.364951 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -0.461215  [   64/60000]\n",
      "loss: -0.491153  [ 6464/60000]\n",
      "loss: -0.494404  [12864/60000]\n",
      "loss: -0.485560  [19264/60000]\n",
      "loss: -0.709281  [25664/60000]\n",
      "loss: -0.675593  [32064/60000]\n",
      "loss: -0.698843  [38464/60000]\n",
      "loss: -0.883810  [44864/60000]\n",
      "loss: -0.722826  [51264/60000]\n",
      "loss: -0.798507  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: -0.810301 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -0.724294  [   64/60000]\n",
      "loss: -0.954240  [ 6464/60000]\n",
      "loss: -0.763945  [12864/60000]\n",
      "loss: -0.802203  [19264/60000]\n",
      "loss: -1.123261  [25664/60000]\n",
      "loss: -1.084765  [32064/60000]\n",
      "loss: -0.871442  [38464/60000]\n",
      "loss: -1.057086  [44864/60000]\n",
      "loss: -1.176418  [51264/60000]\n",
      "loss: -1.489545  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.5%, Avg loss: -1.217430 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -1.318874  [   64/60000]\n",
      "loss: -1.204513  [ 6464/60000]\n",
      "loss: -1.320109  [12864/60000]\n",
      "loss: -1.450780  [19264/60000]\n",
      "loss: -1.332106  [25664/60000]\n",
      "loss: -1.499828  [32064/60000]\n",
      "loss: -1.530696  [38464/60000]\n",
      "loss: -1.592447  [44864/60000]\n",
      "loss: -1.593719  [51264/60000]\n",
      "loss: -1.694653  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: -1.573028 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -1.631053  [   64/60000]\n",
      "loss: -1.621565  [ 6464/60000]\n",
      "loss: -1.647180  [12864/60000]\n",
      "loss: -1.615546  [19264/60000]\n",
      "loss: -1.715786  [25664/60000]\n",
      "loss: -1.554440  [32064/60000]\n",
      "loss: -1.764413  [38464/60000]\n",
      "loss: -1.893844  [44864/60000]\n",
      "loss: -1.660887  [51264/60000]\n",
      "loss: -1.870416  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: -1.915644 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -1.745050  [   64/60000]\n",
      "loss: -1.854642  [ 6464/60000]\n",
      "loss: -2.152846  [12864/60000]\n",
      "loss: -2.073532  [19264/60000]\n",
      "loss: -1.930016  [25664/60000]\n",
      "loss: -2.094130  [32064/60000]\n",
      "loss: -2.304114  [38464/60000]\n",
      "loss: -2.097539  [44864/60000]\n",
      "loss: -2.253722  [51264/60000]\n",
      "loss: -2.040413  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: -2.216132 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -2.124375  [   64/60000]\n",
      "loss: -2.057397  [ 6464/60000]\n",
      "loss: -2.271147  [12864/60000]\n",
      "loss: -2.398753  [19264/60000]\n",
      "loss: -2.377427  [25664/60000]\n",
      "loss: -2.233234  [32064/60000]\n",
      "loss: -2.187258  [38464/60000]\n",
      "loss: -2.265502  [44864/60000]\n",
      "loss: -2.373560  [51264/60000]\n",
      "loss: -2.555682  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: -2.518615 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -2.438945  [   64/60000]\n",
      "loss: -2.595800  [ 6464/60000]\n",
      "loss: -2.323272  [12864/60000]\n",
      "loss: -2.512391  [19264/60000]\n",
      "loss: -2.173116  [25664/60000]\n",
      "loss: -2.205785  [32064/60000]\n",
      "loss: -2.812737  [38464/60000]\n",
      "loss: -2.811605  [44864/60000]\n",
      "loss: -2.490554  [51264/60000]\n",
      "loss: -2.831666  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: -2.737252 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -2.890402  [   64/60000]\n",
      "loss: -2.883823  [ 6464/60000]\n",
      "loss: -2.821987  [12864/60000]\n",
      "loss: -2.390221  [19264/60000]\n",
      "loss: -2.804148  [25664/60000]\n",
      "loss: -3.071900  [32064/60000]\n",
      "loss: -2.680125  [38464/60000]\n",
      "loss: -2.766209  [44864/60000]\n",
      "loss: -3.029191  [51264/60000]\n",
      "loss: -2.732412  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: -2.975906 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -3.144276  [   64/60000]\n",
      "loss: -2.922988  [ 6464/60000]\n",
      "loss: -3.318457  [12864/60000]\n",
      "loss: -3.024863  [19264/60000]\n",
      "loss: -3.247140  [25664/60000]\n",
      "loss: -3.056061  [32064/60000]\n",
      "loss: -3.023531  [38464/60000]\n",
      "loss: -3.031983  [44864/60000]\n",
      "loss: -3.228494  [51264/60000]\n",
      "loss: -3.317594  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: -3.248341 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: -0.047756  [   64/60000]\n",
      "loss: -2.521253  [ 6464/60000]\n",
      "loss: -4.385056  [12864/60000]\n",
      "loss: -7.339796  [19264/60000]\n",
      "loss: -9.826818  [25664/60000]\n",
      "loss: -12.572536  [32064/60000]\n",
      "loss: -14.544263  [38464/60000]\n",
      "loss: -17.207977  [44864/60000]\n",
      "loss: -18.577305  [51264/60000]\n",
      "loss: -24.785332  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: -25.072896 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -24.980991  [   64/60000]\n",
      "loss: -26.109180  [ 6464/60000]\n",
      "loss: -29.996325  [12864/60000]\n",
      "loss: -35.970818  [19264/60000]\n",
      "loss: -39.351364  [25664/60000]\n",
      "loss: -41.675194  [32064/60000]\n",
      "loss: -45.132782  [38464/60000]\n",
      "loss: -56.982292  [44864/60000]\n",
      "loss: -60.024788  [51264/60000]\n",
      "loss: -59.462383  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: -62.515595 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -65.828506  [   64/60000]\n",
      "loss: -64.897217  [ 6464/60000]\n",
      "loss: -70.579201  [12864/60000]\n",
      "loss: -81.427429  [19264/60000]\n",
      "loss: -89.680847  [25664/60000]\n",
      "loss: -93.610741  [32064/60000]\n",
      "loss: -100.545105  [38464/60000]\n",
      "loss: -101.628326  [44864/60000]\n",
      "loss: -108.059891  [51264/60000]\n",
      "loss: -129.018265  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: -126.567822 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -117.003647  [   64/60000]\n",
      "loss: -135.167435  [ 6464/60000]\n",
      "loss: -144.614929  [12864/60000]\n",
      "loss: -147.585678  [19264/60000]\n",
      "loss: -157.277969  [25664/60000]\n",
      "loss: -171.142151  [32064/60000]\n",
      "loss: -183.808167  [38464/60000]\n",
      "loss: -202.675705  [44864/60000]\n",
      "loss: -211.266876  [51264/60000]\n",
      "loss: -225.684631  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: -218.377276 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -226.840393  [   64/60000]\n",
      "loss: -209.807404  [ 6464/60000]\n",
      "loss: -265.126953  [12864/60000]\n",
      "loss: -272.899994  [19264/60000]\n",
      "loss: -288.879211  [25664/60000]\n",
      "loss: -287.441040  [32064/60000]\n",
      "loss: -345.000031  [38464/60000]\n",
      "loss: -329.461487  [44864/60000]\n",
      "loss: -360.947449  [51264/60000]\n",
      "loss: -414.802551  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: -397.839431 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -403.764771  [   64/60000]\n",
      "loss: -411.849304  [ 6464/60000]\n",
      "loss: -458.228699  [12864/60000]\n",
      "loss: -478.231049  [19264/60000]\n",
      "loss: -524.041748  [25664/60000]\n",
      "loss: -494.286804  [32064/60000]\n",
      "loss: -580.260681  [38464/60000]\n",
      "loss: -595.156128  [44864/60000]\n",
      "loss: -646.780518  [51264/60000]\n",
      "loss: -613.822876  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: -708.503071 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -660.337341  [   64/60000]\n",
      "loss: -704.465942  [ 6464/60000]\n",
      "loss: -742.147095  [12864/60000]\n",
      "loss: -829.249878  [19264/60000]\n",
      "loss: -952.787109  [25664/60000]\n",
      "loss: -942.619995  [32064/60000]\n",
      "loss: -1097.654907  [38464/60000]\n",
      "loss: -784.148010  [44864/60000]\n",
      "loss: -1233.428711  [51264/60000]\n",
      "loss: -1098.672729  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.6%, Avg loss: -1251.198598 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -1144.051025  [   64/60000]\n",
      "loss: -1288.946533  [ 6464/60000]\n",
      "loss: -1412.930908  [12864/60000]\n",
      "loss: -1539.136475  [19264/60000]\n",
      "loss: -1497.370850  [25664/60000]\n",
      "loss: -1794.440552  [32064/60000]\n",
      "loss: -1763.151489  [38464/60000]\n",
      "loss: -1767.791992  [44864/60000]\n",
      "loss: -1959.182617  [51264/60000]\n",
      "loss: -2141.153809  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: -2128.282757 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -2080.439453  [   64/60000]\n",
      "loss: -2270.547119  [ 6464/60000]\n",
      "loss: -2428.346191  [12864/60000]\n",
      "loss: -2783.434570  [19264/60000]\n",
      "loss: -2761.924805  [25664/60000]\n",
      "loss: -2920.653564  [32064/60000]\n",
      "loss: -3239.088379  [38464/60000]\n",
      "loss: -3234.443115  [44864/60000]\n",
      "loss: -3663.316895  [51264/60000]\n",
      "loss: -3681.768555  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.5%, Avg loss: -4166.632495 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -2957.660889  [   64/60000]\n",
      "loss: -4046.988770  [ 6464/60000]\n",
      "loss: -4408.568359  [12864/60000]\n",
      "loss: -4575.853516  [19264/60000]\n",
      "loss: -4717.170898  [25664/60000]\n",
      "loss: -5633.113281  [32064/60000]\n",
      "loss: -6277.843750  [38464/60000]\n",
      "loss: -6155.594727  [44864/60000]\n",
      "loss: -6525.148438  [51264/60000]\n",
      "loss: -6632.347656  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: -7020.759974 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.072876  [   64/60000]\n",
      "loss: -6.509774  [ 6464/60000]\n",
      "loss: -9.496752  [12864/60000]\n",
      "loss: -13.197715  [19264/60000]\n",
      "loss: -15.636051  [25664/60000]\n",
      "loss: -18.382252  [32064/60000]\n",
      "loss: -22.322399  [38464/60000]\n",
      "loss: -24.753380  [44864/60000]\n",
      "loss: -29.128983  [51264/60000]\n",
      "loss: -31.940792  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: -33.642930 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -34.723469  [   64/60000]\n",
      "loss: -37.014610  [ 6464/60000]\n",
      "loss: -39.443954  [12864/60000]\n",
      "loss: -43.832722  [19264/60000]\n",
      "loss: -46.056213  [25664/60000]\n",
      "loss: -51.426025  [32064/60000]\n",
      "loss: -56.206284  [38464/60000]\n",
      "loss: -60.175549  [44864/60000]\n",
      "loss: -64.078339  [51264/60000]\n",
      "loss: -68.087906  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: -69.442826 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -71.161797  [   64/60000]\n",
      "loss: -73.431458  [ 6464/60000]\n",
      "loss: -76.766243  [12864/60000]\n",
      "loss: -82.301636  [19264/60000]\n",
      "loss: -88.122406  [25664/60000]\n",
      "loss: -92.835281  [32064/60000]\n",
      "loss: -96.742935  [38464/60000]\n",
      "loss: -101.370941  [44864/60000]\n",
      "loss: -105.188019  [51264/60000]\n",
      "loss: -111.828766  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: -113.180424 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -114.453156  [   64/60000]\n",
      "loss: -121.636856  [ 6464/60000]\n",
      "loss: -125.514709  [12864/60000]\n",
      "loss: -130.623367  [19264/60000]\n",
      "loss: -133.126999  [25664/60000]\n",
      "loss: -143.328384  [32064/60000]\n",
      "loss: -147.168304  [38464/60000]\n",
      "loss: -155.586990  [44864/60000]\n",
      "loss: -159.516724  [51264/60000]\n",
      "loss: -162.415344  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: -173.575591 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -170.712677  [   64/60000]\n",
      "loss: -176.168304  [ 6464/60000]\n",
      "loss: -182.072769  [12864/60000]\n",
      "loss: -188.728424  [19264/60000]\n",
      "loss: -193.599030  [25664/60000]\n",
      "loss: -201.081909  [32064/60000]\n",
      "loss: -211.082199  [38464/60000]\n",
      "loss: -216.644104  [44864/60000]\n",
      "loss: -224.461853  [51264/60000]\n",
      "loss: -218.979080  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: -227.016656 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -228.746628  [   64/60000]\n",
      "loss: -238.089111  [ 6464/60000]\n",
      "loss: -248.112259  [12864/60000]\n",
      "loss: -256.657959  [19264/60000]\n",
      "loss: -260.667755  [25664/60000]\n",
      "loss: -267.109375  [32064/60000]\n",
      "loss: -279.186523  [38464/60000]\n",
      "loss: -283.831238  [44864/60000]\n",
      "loss: -294.660217  [51264/60000]\n",
      "loss: -301.967224  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: -302.408220 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -301.599213  [   64/60000]\n",
      "loss: -313.589417  [ 6464/60000]\n",
      "loss: -320.201172  [12864/60000]\n",
      "loss: -325.733826  [19264/60000]\n",
      "loss: -339.661255  [25664/60000]\n",
      "loss: -347.714844  [32064/60000]\n",
      "loss: -356.990906  [38464/60000]\n",
      "loss: -360.385986  [44864/60000]\n",
      "loss: -369.911652  [51264/60000]\n",
      "loss: -383.756073  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: -386.245111 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -388.481323  [   64/60000]\n",
      "loss: -394.157806  [ 6464/60000]\n",
      "loss: -394.076202  [12864/60000]\n",
      "loss: -410.747986  [19264/60000]\n",
      "loss: -422.749207  [25664/60000]\n",
      "loss: -435.286743  [32064/60000]\n",
      "loss: -438.243958  [38464/60000]\n",
      "loss: -442.970245  [44864/60000]\n",
      "loss: -465.981781  [51264/60000]\n",
      "loss: -468.816406  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: -473.826150 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -479.675781  [   64/60000]\n",
      "loss: -488.654541  [ 6464/60000]\n",
      "loss: -497.946991  [12864/60000]\n",
      "loss: -512.340454  [19264/60000]\n",
      "loss: -519.583984  [25664/60000]\n",
      "loss: -531.891541  [32064/60000]\n",
      "loss: -535.837036  [38464/60000]\n",
      "loss: -551.864014  [44864/60000]\n",
      "loss: -560.372070  [51264/60000]\n",
      "loss: -572.758606  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: -582.428804 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -579.304382  [   64/60000]\n",
      "loss: -577.543213  [ 6464/60000]\n",
      "loss: -599.042603  [12864/60000]\n",
      "loss: -605.633179  [19264/60000]\n",
      "loss: -624.110229  [25664/60000]\n",
      "loss: -630.161621  [32064/60000]\n",
      "loss: -643.671936  [38464/60000]\n",
      "loss: -650.780334  [44864/60000]\n",
      "loss: -673.399353  [51264/60000]\n",
      "loss: -685.038330  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: -681.600995 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.215495  [   64/60000]\n",
      "loss: 0.131803  [ 6464/60000]\n",
      "loss: 0.046670  [12864/60000]\n",
      "loss: 0.065569  [19264/60000]\n",
      "loss: -0.067649  [25664/60000]\n",
      "loss: -0.110848  [32064/60000]\n",
      "loss: -0.053409  [38464/60000]\n",
      "loss: -0.213718  [44864/60000]\n",
      "loss: -0.257917  [51264/60000]\n",
      "loss: -0.349429  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: -0.298552 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: -0.316358  [   64/60000]\n",
      "loss: -0.396698  [ 6464/60000]\n",
      "loss: -0.386109  [12864/60000]\n",
      "loss: -0.501587  [19264/60000]\n",
      "loss: -0.597482  [25664/60000]\n",
      "loss: -0.523587  [32064/60000]\n",
      "loss: -0.592648  [38464/60000]\n",
      "loss: -0.660187  [44864/60000]\n",
      "loss: -0.624726  [51264/60000]\n",
      "loss: -0.678977  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.0%, Avg loss: -0.725266 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: -0.646409  [   64/60000]\n",
      "loss: -0.679052  [ 6464/60000]\n",
      "loss: -0.706304  [12864/60000]\n",
      "loss: -0.957463  [19264/60000]\n",
      "loss: -0.893875  [25664/60000]\n",
      "loss: -0.936276  [32064/60000]\n",
      "loss: -0.997547  [38464/60000]\n",
      "loss: -0.965928  [44864/60000]\n",
      "loss: -1.116612  [51264/60000]\n",
      "loss: -1.042047  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: -1.120923 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: -1.107396  [   64/60000]\n",
      "loss: -1.095466  [ 6464/60000]\n",
      "loss: -1.243134  [12864/60000]\n",
      "loss: -1.243000  [19264/60000]\n",
      "loss: -1.128890  [25664/60000]\n",
      "loss: -1.337813  [32064/60000]\n",
      "loss: -1.298847  [38464/60000]\n",
      "loss: -1.457649  [44864/60000]\n",
      "loss: -1.447819  [51264/60000]\n",
      "loss: -1.383480  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: -1.466172 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: -1.522354  [   64/60000]\n",
      "loss: -1.422191  [ 6464/60000]\n",
      "loss: -1.641440  [12864/60000]\n",
      "loss: -1.538588  [19264/60000]\n",
      "loss: -1.448782  [25664/60000]\n",
      "loss: -1.658636  [32064/60000]\n",
      "loss: -1.605573  [38464/60000]\n",
      "loss: -1.704101  [44864/60000]\n",
      "loss: -1.846351  [51264/60000]\n",
      "loss: -1.914712  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: -1.806073 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: -1.800109  [   64/60000]\n",
      "loss: -1.805816  [ 6464/60000]\n",
      "loss: -1.758021  [12864/60000]\n",
      "loss: -1.902813  [19264/60000]\n",
      "loss: -2.057803  [25664/60000]\n",
      "loss: -1.674637  [32064/60000]\n",
      "loss: -1.847471  [38464/60000]\n",
      "loss: -1.896767  [44864/60000]\n",
      "loss: -2.012509  [51264/60000]\n",
      "loss: -2.242646  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: -2.122138 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: -2.080832  [   64/60000]\n",
      "loss: -2.063289  [ 6464/60000]\n",
      "loss: -2.167324  [12864/60000]\n",
      "loss: -2.330443  [19264/60000]\n",
      "loss: -2.199882  [25664/60000]\n",
      "loss: -2.263137  [32064/60000]\n",
      "loss: -2.134186  [38464/60000]\n",
      "loss: -2.369142  [44864/60000]\n",
      "loss: -2.200646  [51264/60000]\n",
      "loss: -2.433355  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: -2.408151 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: -2.132690  [   64/60000]\n",
      "loss: -2.600509  [ 6464/60000]\n",
      "loss: -2.492953  [12864/60000]\n",
      "loss: -2.605683  [19264/60000]\n",
      "loss: -2.257660  [25664/60000]\n",
      "loss: -2.670269  [32064/60000]\n",
      "loss: -2.409070  [38464/60000]\n",
      "loss: -2.758069  [44864/60000]\n",
      "loss: -2.344390  [51264/60000]\n",
      "loss: -2.526017  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: -2.671730 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: -2.601490  [   64/60000]\n",
      "loss: -2.750179  [ 6464/60000]\n",
      "loss: -2.705546  [12864/60000]\n",
      "loss: -2.443197  [19264/60000]\n",
      "loss: -2.737514  [25664/60000]\n",
      "loss: -2.463429  [32064/60000]\n",
      "loss: -2.841095  [38464/60000]\n",
      "loss: -2.932321  [44864/60000]\n",
      "loss: -2.778057  [51264/60000]\n",
      "loss: -2.746201  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: -2.894704 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: -2.685249  [   64/60000]\n",
      "loss: -3.036510  [ 6464/60000]\n",
      "loss: -2.879579  [12864/60000]\n",
      "loss: -2.997935  [19264/60000]\n",
      "loss: -3.034805  [25664/60000]\n",
      "loss: -3.292402  [32064/60000]\n",
      "loss: -3.237514  [38464/60000]\n",
      "loss: -3.236160  [44864/60000]\n",
      "loss: -3.047812  [51264/60000]\n",
      "loss: -3.389984  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: -3.156300 \n",
      "\n",
      "The min losses loss_fn=NLLLoss2d(), optimizer=<function get_Adadelta at 0x17eef9b20> - min_loss=0.35389127786372115, acc_while_min_loss=87.94999999999999\n"
     ]
    }
   ],
   "source": [
    "acc_while_min_loss = 0\n",
    "min_loss = 100\n",
    "accurate_loss = None\n",
    "accurate_optimizer = None\n",
    "epochs = 10\n",
    "for loss_fn in losses:\n",
    "    for optimizer in optimizers:\n",
    "        model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n",
    "            model,\n",
    "            optimizer,\n",
    "            train_dataloader,\n",
    "            test_dataloader,\n",
    "        )\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train(train_dataloader, model, loss_fn, optimizer(model))\n",
    "            loss, acc = test(test_dataloader, model, loss_fn)\n",
    "        if loss < min_loss:\n",
    "            acc_while_min_loss = acc\n",
    "            min_loss = loss\n",
    "            accurate_loss = loss_fn\n",
    "            accurate_optimizer = optimizer\n",
    "            accurate_model = model\n",
    "\n",
    "print(f\"The min losses {loss_fn=}, {optimizer=} - {min_loss=}, {acc_while_min_loss=}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fab8d7",
   "metadata": {},
   "source": [
    "Задание: придумайте свою собственную модель, на занятии 13 марта сравнимся, у кого получилось достичь наилучших результатов. Для этого мне нужно будет получить как вашу версию этого ноутбука, так и веса модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2e232",
   "metadata": {},
   "source": [
    "При подборе гиперпараметров (это те параметры, которые не обучаются, например, lr) и обучении модели помните:\n",
    "1. Если параметры выбраны хорошо, улучшение видно уже после нескольких сотен итераций.\n",
    "2. Настраивайте гиперпараметры от грубых значений к точным: начните с тестирования большого диапазона гиперпараметров всего за несколько итераций обучения, чтобы найти комбинации параметров, которые вообще работают.\n",
    "3. После того, как вы нашли несколько наборов параметров, которые, по-видимому, работают, выполните более тщательный поиск по этим параметрам. Возможно, вам придется тренировать модель большее число эпох.\n",
    "4. Нужно использовать валидационный набор для поиска. Этот набор нужно выделить из тренировочного и вызывать на нем функцию test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "517c8098-42da-4ce3-895b-f171538cd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FashionMNISTDataset(\n",
    "    file_dir=\"data/FashionMNIST/raw\",\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "test_data = FashionMNISTDataset(\n",
    "    file_dir=\"data/FashionMNIST/raw\",\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "538681c4-20a2-4b9f-b75e-6fac4ff79ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2, momentum=0.2)    \n",
    "epochs = 10\n",
    "model, optimizer, train_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "72ee8f83-1b8f-4637-ab5e-ac66be721877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.069219  [   64/60000]\n",
      "loss: 0.144767  [ 6464/60000]\n",
      "loss: 0.144928  [12864/60000]\n",
      "loss: 0.098603  [19264/60000]\n",
      "loss: 0.095550  [25664/60000]\n",
      "loss: 0.117936  [32064/60000]\n",
      "loss: 0.100972  [38464/60000]\n",
      "loss: 0.056980  [44864/60000]\n",
      "loss: 0.029585  [51264/60000]\n",
      "loss: 0.151361  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.428134 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.098034  [   64/60000]\n",
      "loss: 0.151875  [ 6464/60000]\n",
      "loss: 0.032457  [12864/60000]\n",
      "loss: 0.083498  [19264/60000]\n",
      "loss: 0.028196  [25664/60000]\n",
      "loss: 0.073600  [32064/60000]\n",
      "loss: 0.065717  [38464/60000]\n",
      "loss: 0.095499  [44864/60000]\n",
      "loss: 0.080213  [51264/60000]\n",
      "loss: 0.033967  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.387752 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.012891  [   64/60000]\n",
      "loss: 0.196331  [ 6464/60000]\n",
      "loss: 0.100591  [12864/60000]\n",
      "loss: 0.095754  [19264/60000]\n",
      "loss: 0.047626  [25664/60000]\n",
      "loss: 0.078213  [32064/60000]\n",
      "loss: 0.087213  [38464/60000]\n",
      "loss: 0.082554  [44864/60000]\n",
      "loss: 0.064630  [51264/60000]\n",
      "loss: 0.055362  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.417090 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.020485  [   64/60000]\n",
      "loss: 0.066396  [ 6464/60000]\n",
      "loss: 0.074438  [12864/60000]\n",
      "loss: 0.064960  [19264/60000]\n",
      "loss: 0.088214  [25664/60000]\n",
      "loss: 0.048994  [32064/60000]\n",
      "loss: 0.109322  [38464/60000]\n",
      "loss: 0.052252  [44864/60000]\n",
      "loss: 0.068463  [51264/60000]\n",
      "loss: 0.097843  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.426360 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.027155  [   64/60000]\n",
      "loss: 0.061868  [ 6464/60000]\n",
      "loss: 0.026540  [12864/60000]\n",
      "loss: 0.045198  [19264/60000]\n",
      "loss: 0.074225  [25664/60000]\n",
      "loss: 0.082736  [32064/60000]\n",
      "loss: 0.024205  [38464/60000]\n",
      "loss: 0.178708  [44864/60000]\n",
      "loss: 0.065476  [51264/60000]\n",
      "loss: 0.137639  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.448987 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.050115  [   64/60000]\n",
      "loss: 0.061950  [ 6464/60000]\n",
      "loss: 0.064667  [12864/60000]\n",
      "loss: 0.050144  [19264/60000]\n",
      "loss: 0.057708  [25664/60000]\n",
      "loss: 0.050259  [32064/60000]\n",
      "loss: 0.093502  [38464/60000]\n",
      "loss: 0.049043  [44864/60000]\n",
      "loss: 0.068383  [51264/60000]\n",
      "loss: 0.064809  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.428316 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.037211  [   64/60000]\n",
      "loss: 0.012650  [ 6464/60000]\n",
      "loss: 0.061177  [12864/60000]\n",
      "loss: 0.109364  [19264/60000]\n",
      "loss: 0.037259  [25664/60000]\n",
      "loss: 0.032603  [32064/60000]\n",
      "loss: 0.038381  [38464/60000]\n",
      "loss: 0.078794  [44864/60000]\n",
      "loss: 0.047482  [51264/60000]\n",
      "loss: 0.026280  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.436371 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.125213  [   64/60000]\n",
      "loss: 0.064464  [ 6464/60000]\n",
      "loss: 0.059617  [12864/60000]\n",
      "loss: 0.060971  [19264/60000]\n",
      "loss: 0.064335  [25664/60000]\n",
      "loss: 0.047854  [32064/60000]\n",
      "loss: 0.023975  [38464/60000]\n",
      "loss: 0.164853  [44864/60000]\n",
      "loss: 0.035145  [51264/60000]\n",
      "loss: 0.008841  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.434079 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.032374  [   64/60000]\n",
      "loss: 0.066231  [ 6464/60000]\n",
      "loss: 0.018520  [12864/60000]\n",
      "loss: 0.046169  [19264/60000]\n",
      "loss: 0.040415  [25664/60000]\n",
      "loss: 0.046562  [32064/60000]\n",
      "loss: 0.045079  [38464/60000]\n",
      "loss: 0.034073  [44864/60000]\n",
      "loss: 0.036738  [51264/60000]\n",
      "loss: 0.075258  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.443089 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.086803  [   64/60000]\n",
      "loss: 0.054129  [ 6464/60000]\n",
      "loss: 0.033039  [12864/60000]\n",
      "loss: 0.068404  [19264/60000]\n",
      "loss: 0.114244  [25664/60000]\n",
      "loss: 0.047626  [32064/60000]\n",
      "loss: 0.035246  [38464/60000]\n",
      "loss: 0.038939  [44864/60000]\n",
      "loss: 0.056223  [51264/60000]\n",
      "loss: 0.054043  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.452772 \n",
      "\n",
      "The min losses loss_fn=CrossEntropyLoss(), optimizer=AcceleratedOptimizer (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.2\n",
      "    maximize: False\n",
      "    momentum: 0.2\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ") - loss=0.4527723666304236, acc=90.25999999999999\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    loss, acc = test(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(f\"The min losses {loss_fn=}, {optimizer=} - {loss=}, {acc=}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07085ea9-ffe9-4392-8371-71ee3288cd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
